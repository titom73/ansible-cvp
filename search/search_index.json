{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ansible Modules for Arista CloudVision Platform # All the CV communication are now managed by cvprac library . So a new requirements MUST be installed first before any code execution. About # Arista Networks supports Ansible for managing devices running the EOS operating system through CloudVision platform (CVP) . This roles includes a set of ansible modules that perform specific configuration tasks on CVP server. These tasks include: collecting facts, managing configlets, containers, build provisionning topology and running tasks. Requirements # Arista CloudVision: CVP 2018.x.x : starting version ansible-cvp 1.0.0 CVP 2019.x.x : starting version ansible-cvp 1.0.0 CVP 2020.1.x : starting version ansible-cvp 1.1.0 CVP 2020.2.x : starting version ansible-cvp 2.0.0 Python: Python 3.x Additional Python Libraries required: cvprac version 1.0.4 requests >= 2.22.0 treelib version 1.5.5 or later Supported Ansible Versions: ansible 2.9 or later Installation # pip install requests> = 2 .22.0 pip install treelib> = 1 .5.5 pip install cvprac == 1 .0.4 Ansible galaxy hosts all stable version of this collection. Installation from ansible-galaxy is the most convenient approach for consuming arista.cvp content $ ansible-galaxy collection install arista.cvp Process install dependency map Starting collection install process Installing 'arista.cvp:1.1.0' to '~/.ansible/collections/ansible_collections/arista/cvp' Complete installation process is available on repository website Modules overview # This repository provides content for Ansible\u2019s collection arista.cvp with following content: List of available modules: arista.cvp.cv_facts - Collect CVP facts from server like list of containers, devices, configlet and tasks. arista.cvp.cv_configlet - Manage configlet configured on CVP. arista.cvp.cv_container - Manage container topology and attach configlet and devices to containers. arista.cvp.cv_device - Manage devices configured on CVP arista.cvp.cv_task - Run tasks created on CVP. List of available roles: arista.cvp.dhcp_configuration - Configure DHCPD service on a Cloudvision server or any dhcpd service. arista.cvp.configlet_sync - Synchronize configlets between multiple Cloudvision servers. Example # This example outlines how to use arista.cvp to create a containers topology on Arista CloudVision. A dedicated repository is available for step by step examples on ansible-cvp-toi . A complete end to end demo using Arista Validated Design collection and CloudVision modules is available as an example. Below is a very basic example to build a container topology on a CloudVision platform assuming you have 3 veos named veos0{1,3} and a configlet named alias --- - name : Playbook to demonstrate cv_container module. hosts : cvp connection : local gather_facts : no collections : - arista.cvp vars : containers_provision : Fabric : parent_container : Tenant Spines : parent_container : Fabric Leaves : parent_container : Fabric configlets : - alias devices : - veos03 MLAG01 : parent_container : Leaves devices : - veos01 - veos02 tasks : - name : \"Gather CVP facts from {{inventory_hostname}}\" cv_facts : register : cvp_facts - name : \"Build Container topology on {{inventory_hostname}}\" cv_container : topology : '{{containers_provision}}' cvp_facts : '{{cvp_facts.ansible_facts}}' As modules of this collection are based on HTTPAPI connection plugin , authentication elements shall be declared using this plugin mechanism and are automatically shared with arista.cvp.cv_* modules. [development] cvp_foster ansible_host = 10.90.224.122 ansible_httpapi_host=10.90.224.122 [development:vars] ansible_connection = httpapi ansible_httpapi_use_ssl = True ansible_httpapi_validate_certs = False ansible_user = cvpadmin ansible_password = ansible ansible_network_os = eos ansible_httpapi_port = 443 As modules of this collection are based on HTTPAPI connection plugin , authentication elements shall be declared using this plugin mechanism and are automatically shared with arista.cvp.cv_* modules. License # Project is published under Apache License . Ask a question # Support for this arista.cvp collection is provided by the community directly in this repository. Easiest way to get support is to open an issue . Contributing # Contributing pull requests are gladly welcomed for this repository. If you are planning a big change, please start a discussion first to make sure we\u2019ll be able to merge it. You can also open an issue to report any problem or to submit enhancement. A more complete guide for contribution is available in the repository","title":"Home"},{"location":"#ansible-modules-for-arista-cloudvision-platform","text":"All the CV communication are now managed by cvprac library . So a new requirements MUST be installed first before any code execution.","title":"Ansible Modules for Arista CloudVision Platform"},{"location":"#about","text":"Arista Networks supports Ansible for managing devices running the EOS operating system through CloudVision platform (CVP) . This roles includes a set of ansible modules that perform specific configuration tasks on CVP server. These tasks include: collecting facts, managing configlets, containers, build provisionning topology and running tasks.","title":"About"},{"location":"#requirements","text":"Arista CloudVision: CVP 2018.x.x : starting version ansible-cvp 1.0.0 CVP 2019.x.x : starting version ansible-cvp 1.0.0 CVP 2020.1.x : starting version ansible-cvp 1.1.0 CVP 2020.2.x : starting version ansible-cvp 2.0.0 Python: Python 3.x Additional Python Libraries required: cvprac version 1.0.4 requests >= 2.22.0 treelib version 1.5.5 or later Supported Ansible Versions: ansible 2.9 or later","title":"Requirements"},{"location":"#installation","text":"pip install requests> = 2 .22.0 pip install treelib> = 1 .5.5 pip install cvprac == 1 .0.4 Ansible galaxy hosts all stable version of this collection. Installation from ansible-galaxy is the most convenient approach for consuming arista.cvp content $ ansible-galaxy collection install arista.cvp Process install dependency map Starting collection install process Installing 'arista.cvp:1.1.0' to '~/.ansible/collections/ansible_collections/arista/cvp' Complete installation process is available on repository website","title":"Installation"},{"location":"#modules-overview","text":"This repository provides content for Ansible\u2019s collection arista.cvp with following content: List of available modules: arista.cvp.cv_facts - Collect CVP facts from server like list of containers, devices, configlet and tasks. arista.cvp.cv_configlet - Manage configlet configured on CVP. arista.cvp.cv_container - Manage container topology and attach configlet and devices to containers. arista.cvp.cv_device - Manage devices configured on CVP arista.cvp.cv_task - Run tasks created on CVP. List of available roles: arista.cvp.dhcp_configuration - Configure DHCPD service on a Cloudvision server or any dhcpd service. arista.cvp.configlet_sync - Synchronize configlets between multiple Cloudvision servers.","title":"Modules overview"},{"location":"#example","text":"This example outlines how to use arista.cvp to create a containers topology on Arista CloudVision. A dedicated repository is available for step by step examples on ansible-cvp-toi . A complete end to end demo using Arista Validated Design collection and CloudVision modules is available as an example. Below is a very basic example to build a container topology on a CloudVision platform assuming you have 3 veos named veos0{1,3} and a configlet named alias --- - name : Playbook to demonstrate cv_container module. hosts : cvp connection : local gather_facts : no collections : - arista.cvp vars : containers_provision : Fabric : parent_container : Tenant Spines : parent_container : Fabric Leaves : parent_container : Fabric configlets : - alias devices : - veos03 MLAG01 : parent_container : Leaves devices : - veos01 - veos02 tasks : - name : \"Gather CVP facts from {{inventory_hostname}}\" cv_facts : register : cvp_facts - name : \"Build Container topology on {{inventory_hostname}}\" cv_container : topology : '{{containers_provision}}' cvp_facts : '{{cvp_facts.ansible_facts}}' As modules of this collection are based on HTTPAPI connection plugin , authentication elements shall be declared using this plugin mechanism and are automatically shared with arista.cvp.cv_* modules. [development] cvp_foster ansible_host = 10.90.224.122 ansible_httpapi_host=10.90.224.122 [development:vars] ansible_connection = httpapi ansible_httpapi_use_ssl = True ansible_httpapi_validate_certs = False ansible_user = cvpadmin ansible_password = ansible ansible_network_os = eos ansible_httpapi_port = 443 As modules of this collection are based on HTTPAPI connection plugin , authentication elements shall be declared using this plugin mechanism and are automatically shared with arista.cvp.cv_* modules.","title":"Example"},{"location":"#license","text":"Project is published under Apache License .","title":"License"},{"location":"#ask-a-question","text":"Support for this arista.cvp collection is provided by the community directly in this repository. Easiest way to get support is to open an issue .","title":"Ask a question"},{"location":"#contributing","text":"Contributing pull requests are gladly welcomed for this repository. If you are planning a big change, please start a discussion first to make sure we\u2019ll be able to merge it. You can also open an issue to report any problem or to submit enhancement. A more complete guide for contribution is available in the repository","title":"Contributing"},{"location":"docs/contributing/","text":"Contribute to Arista ansible-cvp collection # Contribute to Arista ansible-cvp collection Reporting Bugs Feature Requests Using the issue tracker Branches Current active branches Pull requests Please take a moment to review this document in order to make the contribution process easy and effective for everyone involved. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, they should reciprocate that respect in addressing your issue or assessing patches and features. Reporting Bugs # First, ensure that you\u2019ve installed the latest stable version of ansible-cvp . If you\u2019re running an older version, it\u2019s possible that the bug has already been fixed. Next, check the GitHub issues list to see if the bug you\u2019ve found has already been reported. If you think you may be experiencing a reported issue that hasn\u2019t already been resolved, please click \u201cadd a reaction\u201d in the top right corner of the issue and add a thumbs up (+1). You might also want to add a comment describing how it\u2019s affecting your installation. This will allow us to prioritize bugs based on how many users are affected. If you haven\u2019t found an existing issue that describes your suspected bug, Do not file an issue until you have received confirmation that it is in fact a bug. Invalid issues are very distracting and slow the pace at which ansible-cvp is developed. When submitting an issue, please be as descriptive as possible. Be sure to include: * The environment in which ansible-cvp is running * The exact steps that can be taken to reproduce the issue (if applicable) * Any error messages generated * Screenshots (if applicable) Please avoid prepending any sort of tag (e.g. \u201c[Bug]\u201d) to the issue title. The issue will be reviewed by a moderator after submission and the appropriate labels will be applied for categorization. Keep in mind that we prioritize bugs based on their severity and how much work is required to resolve them. It may take some time for someone to address your issue. Feature Requests # First, check the GitHub issues list to see if the feature you\u2019re requesting is already listed. (Be sure to search closed issues as well, since some feature requests have been rejected.) If the feature you\u2019d like to see has already been requested and is open, click \u201cadd a reaction\u201d in the top right corner of the issue and add a thumbs up (+1). This ensures that the issue has a better chance of receiving attention. Also feel free to add a comment with any additional justification for the feature. (However, note that comments with no substance other than a \u201c+1\u201d will be deleted. Please use GitHub\u2019s reactions feature to indicate your support.) Before filing a new feature request, consider raising your idea on the mailing list first. Feedback you receive there will help validate and shape the proposed feature before filing a formal issue. Good feature requests are very narrowly defined. Be sure to thoroughly describe the functionality and data model(s) being proposed. The more effort you put into writing a feature request, the better its chance is of being implemented. Overly broad feature requests will be closed. When submitting a feature request on GitHub, be sure to include the following: * A detailed description of the proposed functionality * A use case for the feature; who would use it and what value it would add to ansible-cvp * A rough description of changes necessary * Any third-party libraries or other resources which would be involved Please avoid prepending any sort of tag (e.g. \u201c[Feature]\u201d) to the issue title. The issue will be reviewed by a moderator after submission and the appropriate labels will be applied for categorization. Using the issue tracker # The issue tracker is the preferred channel for bug reports , features requests and submitting pull requests , but please respect the following restrictions: Please do not use the issue tracker for personal support requests. Please do not derail or troll issues. Keep the discussion on topic and respect the opinions of others. Branches # Current active branches # Current development branch: devel Stable branch: releases/v1.1.x Pull requests # Be sure to open an issue before starting work on a pull request, and discuss your idea with the ansible-cvp maintainers before beginning work. This will help prevent wasting time on something that might we might not be able to implement. When suggesting a new feature, also make sure it won\u2019t conflict with any work that\u2019s already in progress. Any pull request which does not relate to an accepted issue will be closed. All major new functionality must include relevant tests where applicable. When submitting a pull request, please be sure to work off of the releases/grant-v1.x branch, rather than master . The releases/grant-v1.x branch is used for ongoing development, while master is used for tagging new stable releases. All code submissions should meet the following criteria (CI will enforce these checks): * Python syntax is valid * All tests pass when run with make sanity * PEP 8 compliance is enforced, with the exception that lines may be greater than 80 characters in length Adhering to the following this process is the best way to get your work merged: Fork the repo, clone your fork, and configure the remotes: # Clone your fork of the repo into the current directory git clone https://github.com/<your-username>/ansible-cvp # Navigate to the newly cloned directory cd ansible-cvp # Assign the original repo to a remote called \"upstream\" git remote add upstream https://github.com/aristanetworks/ansible-cvp.git If you cloned a while ago, get the latest changes from upstream: git checkout <dev-branch> git pull upstream <dev-branch> Please refer to branches section to get current branch to use as <dev-branch> Create a new topic branch (off the main project development branch) to contain your feature, change, or fix: git checkout -b <topic-branch-name> Commit your changes in logical chunks. Please adhere to these git commit message guidelines or your code is unlikely be merged into the main project. Use Git\u2019s interactive rebase feature to tidy up your commits before making them public. Locally merge (or rebase) the upstream development branch into your topic branch: git pull [ --rebase ] upstream <dev-branch> Push your topic branch up to your fork: git push origin <topic-branch-name> Open a Pull Request with a clear title and description.","title":"Contribute to Arista ansible-cvp collection"},{"location":"docs/contributing/#contribute-to-arista-ansible-cvp-collection","text":"Contribute to Arista ansible-cvp collection Reporting Bugs Feature Requests Using the issue tracker Branches Current active branches Pull requests Please take a moment to review this document in order to make the contribution process easy and effective for everyone involved. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, they should reciprocate that respect in addressing your issue or assessing patches and features.","title":"Contribute to Arista ansible-cvp collection"},{"location":"docs/contributing/#reporting-bugs","text":"First, ensure that you\u2019ve installed the latest stable version of ansible-cvp . If you\u2019re running an older version, it\u2019s possible that the bug has already been fixed. Next, check the GitHub issues list to see if the bug you\u2019ve found has already been reported. If you think you may be experiencing a reported issue that hasn\u2019t already been resolved, please click \u201cadd a reaction\u201d in the top right corner of the issue and add a thumbs up (+1). You might also want to add a comment describing how it\u2019s affecting your installation. This will allow us to prioritize bugs based on how many users are affected. If you haven\u2019t found an existing issue that describes your suspected bug, Do not file an issue until you have received confirmation that it is in fact a bug. Invalid issues are very distracting and slow the pace at which ansible-cvp is developed. When submitting an issue, please be as descriptive as possible. Be sure to include: * The environment in which ansible-cvp is running * The exact steps that can be taken to reproduce the issue (if applicable) * Any error messages generated * Screenshots (if applicable) Please avoid prepending any sort of tag (e.g. \u201c[Bug]\u201d) to the issue title. The issue will be reviewed by a moderator after submission and the appropriate labels will be applied for categorization. Keep in mind that we prioritize bugs based on their severity and how much work is required to resolve them. It may take some time for someone to address your issue.","title":"Reporting Bugs"},{"location":"docs/contributing/#feature-requests","text":"First, check the GitHub issues list to see if the feature you\u2019re requesting is already listed. (Be sure to search closed issues as well, since some feature requests have been rejected.) If the feature you\u2019d like to see has already been requested and is open, click \u201cadd a reaction\u201d in the top right corner of the issue and add a thumbs up (+1). This ensures that the issue has a better chance of receiving attention. Also feel free to add a comment with any additional justification for the feature. (However, note that comments with no substance other than a \u201c+1\u201d will be deleted. Please use GitHub\u2019s reactions feature to indicate your support.) Before filing a new feature request, consider raising your idea on the mailing list first. Feedback you receive there will help validate and shape the proposed feature before filing a formal issue. Good feature requests are very narrowly defined. Be sure to thoroughly describe the functionality and data model(s) being proposed. The more effort you put into writing a feature request, the better its chance is of being implemented. Overly broad feature requests will be closed. When submitting a feature request on GitHub, be sure to include the following: * A detailed description of the proposed functionality * A use case for the feature; who would use it and what value it would add to ansible-cvp * A rough description of changes necessary * Any third-party libraries or other resources which would be involved Please avoid prepending any sort of tag (e.g. \u201c[Feature]\u201d) to the issue title. The issue will be reviewed by a moderator after submission and the appropriate labels will be applied for categorization.","title":"Feature Requests"},{"location":"docs/contributing/#using-the-issue-tracker","text":"The issue tracker is the preferred channel for bug reports , features requests and submitting pull requests , but please respect the following restrictions: Please do not use the issue tracker for personal support requests. Please do not derail or troll issues. Keep the discussion on topic and respect the opinions of others.","title":"Using the issue tracker"},{"location":"docs/contributing/#branches","text":"","title":"Branches"},{"location":"docs/contributing/#current-active-branches","text":"Current development branch: devel Stable branch: releases/v1.1.x","title":"Current active branches"},{"location":"docs/contributing/#pull-requests","text":"Be sure to open an issue before starting work on a pull request, and discuss your idea with the ansible-cvp maintainers before beginning work. This will help prevent wasting time on something that might we might not be able to implement. When suggesting a new feature, also make sure it won\u2019t conflict with any work that\u2019s already in progress. Any pull request which does not relate to an accepted issue will be closed. All major new functionality must include relevant tests where applicable. When submitting a pull request, please be sure to work off of the releases/grant-v1.x branch, rather than master . The releases/grant-v1.x branch is used for ongoing development, while master is used for tagging new stable releases. All code submissions should meet the following criteria (CI will enforce these checks): * Python syntax is valid * All tests pass when run with make sanity * PEP 8 compliance is enforced, with the exception that lines may be greater than 80 characters in length Adhering to the following this process is the best way to get your work merged: Fork the repo, clone your fork, and configure the remotes: # Clone your fork of the repo into the current directory git clone https://github.com/<your-username>/ansible-cvp # Navigate to the newly cloned directory cd ansible-cvp # Assign the original repo to a remote called \"upstream\" git remote add upstream https://github.com/aristanetworks/ansible-cvp.git If you cloned a while ago, get the latest changes from upstream: git checkout <dev-branch> git pull upstream <dev-branch> Please refer to branches section to get current branch to use as <dev-branch> Create a new topic branch (off the main project development branch) to contain your feature, change, or fix: git checkout -b <topic-branch-name> Commit your changes in logical chunks. Please adhere to these git commit message guidelines or your code is unlikely be merged into the main project. Use Git\u2019s interactive rebase feature to tidy up your commits before making them public. Locally merge (or rebase) the upstream development branch into your topic branch: git pull [ --rebase ] upstream <dev-branch> Push your topic branch up to your fork: git push origin <topic-branch-name> Open a Pull Request with a clear title and description.","title":"Pull requests"},{"location":"docs/getting-started/","text":"Getting Started # This example outlines how to use arista.cvp to create a containers topology on Arista CloudVision. A complete end to end demo using Arista Validated Design collection and CloudVision modules is available as an example. Below is a very basic example to build a container topology on a CloudVision platform assuming you have 3 veos named veos0{1,3} and a configlet named alias --- - name : Playbook to demonstrate cv_container module. hosts : cvp connection : local gather_facts : no collections : - arista.cvp vars : containers_provision : Fabric : parent_container : Tenant Spines : parent_container : Fabric Leaves : parent_container : Fabric configlets : - alias devices : - veos03 MLAG01 : parent_container : Leaves devices : - veos01 - veos02 tasks : - name : \"Gather CVP facts from {{inventory_hostname}}\" cv_facts : register : cvp_facts - name : \"Build Container topology on {{inventory_hostname}}\" cv_container : topology : '{{containers_provision}}' cvp_facts : '{{cvp_facts.ansible_facts}}' As modules of this collection are based on HTTPAPI connection plugin , authentication elements shall be declared using this plugin mechanism and are automatically shared with arista.cvp.cv_* modules. [development] cvp_foster ansible_host = 10.90.224.122 ansible_httpapi_host=10.90.224.122 [development:vars] ansible_connection = httpapi ansible_httpapi_use_ssl = True ansible_httpapi_validate_certs = False ansible_user = cvpadmin ansible_password = ansible ansible_network_os = eos ansible_httpapi_port = 443 As modules of this collection are based on HTTPAPI connection plugin , authentication elements shall be declared using this plugin mechanism and are automatically shared with arista.cvp.cv_* modules.","title":"Getting Started"},{"location":"docs/getting-started/#getting-started","text":"This example outlines how to use arista.cvp to create a containers topology on Arista CloudVision. A complete end to end demo using Arista Validated Design collection and CloudVision modules is available as an example. Below is a very basic example to build a container topology on a CloudVision platform assuming you have 3 veos named veos0{1,3} and a configlet named alias --- - name : Playbook to demonstrate cv_container module. hosts : cvp connection : local gather_facts : no collections : - arista.cvp vars : containers_provision : Fabric : parent_container : Tenant Spines : parent_container : Fabric Leaves : parent_container : Fabric configlets : - alias devices : - veos03 MLAG01 : parent_container : Leaves devices : - veos01 - veos02 tasks : - name : \"Gather CVP facts from {{inventory_hostname}}\" cv_facts : register : cvp_facts - name : \"Build Container topology on {{inventory_hostname}}\" cv_container : topology : '{{containers_provision}}' cvp_facts : '{{cvp_facts.ansible_facts}}' As modules of this collection are based on HTTPAPI connection plugin , authentication elements shall be declared using this plugin mechanism and are automatically shared with arista.cvp.cv_* modules. [development] cvp_foster ansible_host = 10.90.224.122 ansible_httpapi_host=10.90.224.122 [development:vars] ansible_connection = httpapi ansible_httpapi_use_ssl = True ansible_httpapi_validate_certs = False ansible_user = cvpadmin ansible_password = ansible ansible_network_os = eos ansible_httpapi_port = 443 As modules of this collection are based on HTTPAPI connection plugin , authentication elements shall be declared using this plugin mechanism and are automatically shared with arista.cvp.cv_* modules.","title":"Getting Started"},{"location":"docs/how-to/cv_configlet/","text":"Configure configlets on Cloudvision # cv_configlet manage configlets on CloudVision: Configlets creation Configlets update Configlets deletion The cv_configlet actions are based on cv_facts results: Use intend approach No declarative action To import content from text file, leverage template rendering and then load from file: use lookup() command Inputs # Full documentation available in module section and a lab is available in the following repository Input variables # CVP_CONFIGLETS : TEAM01-alias : \"alias a1 show version\" TEAM01-another-configlet : \"alias a2 show version\" Module inputs # Required Inputs # cvp_facts : Facts from cv_facts configlets : List of configlets to create configlet_filter : A filter to target only specific configlets on CV Optional inputs # state : Keyword to define if we want to create(present) or delete(absent) configlets --- - name : lab03 - cv_configlet lab hosts : CloudVision connection : local gather_facts : no vars : CVP_CONFIGLETS : TEAM01-alias : \"alias a1 show version\" TEAM01-another-configlet : \"alias a2 show version\" tasks : - name : \"Gather CVP facts {{inventory_hostname}}\" arista.cvp.cv_facts : register : CVP_FACTS - name : \"Configure configlet on {{inventory_hostname}}\" arista.cvp.cv_configlet : cvp_facts : \"{{CVP_FACTS.ansible_facts}}\" configlets : \"{{CVP_CONFIGLETS}}\" configlet_filter : [ \"TEAM01\" ] state : present Module outputs # cv_configlet outputs: List of created configlets List of updated configlets List of deleted configlets List of generated tasks. ok: [ CloudVision ] => { \"msg\" : { \"changed\" : true , \"data\" : { \"deleted\" : [], \"new\" : [ { \"TEAM01-alias\" : \"success\" } ], \"tasks\" : [], \"updated\" : [] }, \"failed\" : false } }","title":"Manage configlets"},{"location":"docs/how-to/cv_configlet/#configure-configlets-on-cloudvision","text":"cv_configlet manage configlets on CloudVision: Configlets creation Configlets update Configlets deletion The cv_configlet actions are based on cv_facts results: Use intend approach No declarative action To import content from text file, leverage template rendering and then load from file: use lookup() command","title":"Configure configlets on Cloudvision"},{"location":"docs/how-to/cv_configlet/#inputs","text":"Full documentation available in module section and a lab is available in the following repository","title":"Inputs"},{"location":"docs/how-to/cv_configlet/#input-variables","text":"CVP_CONFIGLETS : TEAM01-alias : \"alias a1 show version\" TEAM01-another-configlet : \"alias a2 show version\"","title":"Input variables"},{"location":"docs/how-to/cv_configlet/#module-inputs","text":"","title":"Module inputs"},{"location":"docs/how-to/cv_configlet/#required-inputs","text":"cvp_facts : Facts from cv_facts configlets : List of configlets to create configlet_filter : A filter to target only specific configlets on CV","title":"Required Inputs"},{"location":"docs/how-to/cv_configlet/#optional-inputs","text":"state : Keyword to define if we want to create(present) or delete(absent) configlets --- - name : lab03 - cv_configlet lab hosts : CloudVision connection : local gather_facts : no vars : CVP_CONFIGLETS : TEAM01-alias : \"alias a1 show version\" TEAM01-another-configlet : \"alias a2 show version\" tasks : - name : \"Gather CVP facts {{inventory_hostname}}\" arista.cvp.cv_facts : register : CVP_FACTS - name : \"Configure configlet on {{inventory_hostname}}\" arista.cvp.cv_configlet : cvp_facts : \"{{CVP_FACTS.ansible_facts}}\" configlets : \"{{CVP_CONFIGLETS}}\" configlet_filter : [ \"TEAM01\" ] state : present","title":"Optional inputs"},{"location":"docs/how-to/cv_configlet/#module-outputs","text":"cv_configlet outputs: List of created configlets List of updated configlets List of deleted configlets List of generated tasks. ok: [ CloudVision ] => { \"msg\" : { \"changed\" : true , \"data\" : { \"deleted\" : [], \"new\" : [ { \"TEAM01-alias\" : \"success\" } ], \"tasks\" : [], \"updated\" : [] }, \"failed\" : false } }","title":"Module outputs"},{"location":"docs/how-to/cv_container/","text":"Configure container on Cloudvision # cv_container manage containers on CloudVision: Support creation/deletion of containers Support devices binding to containers Support configlets binding to containers cv_container bases its actions on cv_facts results The cv_container actions are based on cv_facts results: Use intend approach No declarative action Inputs # Full documentation available in module section and a lab is available in the following repository Input variables # Container Name Parent container where to create container Optional list of devices to attach to container - Devices must be already registered - Should not be in undefined container Optional list of configlets to attach to container: - Configlets must be created previously --- CVP_CONTAINERS : TEAM01 : parent_container : Tenant TEAM01_DC : parent_container : TEAM01 TEAM01_LEAFS : parent_container : TEAM01_DC configlets : - GLOBAL-ALIASES TEAM01_SPINES : parent_container : TEAM01_DC devices : - sw01 - sw02 Module inputs # Required Inputs # cvp_facts : Facts from cv_facts topology : Container topology Optional inputs # mode : Define how to manage container available on CV and not in customer topology - merge (default) - delete - override - name : lab04 - cv_container lab hosts : CloudVision connection : local gather_facts : no tasks : - name : \"Gather CVP facts {{inventory_hostname}}\" arista.cvp.cv_facts : register : CVP_FACTS - name : \"Configure containers on {{inventory_hostname}}\" arista.cvp.cv_container : cvp_facts : \"{{CVP_FACTS.ansible_facts}}\" topology : \"{{CVP_CONTAINERS}}\" Module output # cv_container returns : List of created containers List of deleted containers List of moved devices List of attached configlets List of CV tasks generated Generated tasks can be consumed directly by cv_tasks. { \"changed\" : false , \"data\" : { \"attached_configlet\" : { \"configlet_attached\" : 4 , \"list\" : [ [ { \"config\" : \"alias v10 show version\" , \"containers\" : [ ], \"devices\" : [ ], \"key\" : \"configlet_885_1325820320363417\" , \"name\" : \"alias\" , \"type\" : \"Static\" } ] ], \"taskIds\" : [ \"127\" ] }, \"changed\" : true , \"creation_result\" : { \"containers_created\" : \"4\" }, \"deletion_result\" : { \"containers_deleted\" : \"1\" }, \"moved_result\" : { \"devices_moved\" : 3 , \"list\" : [ \"veos01\" , \"veos02\" , \"veos03\" ], \"taskIds\" : [ \"125\" , \"126\" , \"127\" ] }, \"tasks\" : [] }, \"failed\" : false }","title":"Manage containers"},{"location":"docs/how-to/cv_container/#configure-container-on-cloudvision","text":"cv_container manage containers on CloudVision: Support creation/deletion of containers Support devices binding to containers Support configlets binding to containers cv_container bases its actions on cv_facts results The cv_container actions are based on cv_facts results: Use intend approach No declarative action","title":"Configure container on Cloudvision"},{"location":"docs/how-to/cv_container/#inputs","text":"Full documentation available in module section and a lab is available in the following repository","title":"Inputs"},{"location":"docs/how-to/cv_container/#input-variables","text":"Container Name Parent container where to create container Optional list of devices to attach to container - Devices must be already registered - Should not be in undefined container Optional list of configlets to attach to container: - Configlets must be created previously --- CVP_CONTAINERS : TEAM01 : parent_container : Tenant TEAM01_DC : parent_container : TEAM01 TEAM01_LEAFS : parent_container : TEAM01_DC configlets : - GLOBAL-ALIASES TEAM01_SPINES : parent_container : TEAM01_DC devices : - sw01 - sw02","title":"Input variables"},{"location":"docs/how-to/cv_container/#module-inputs","text":"","title":"Module inputs"},{"location":"docs/how-to/cv_container/#required-inputs","text":"cvp_facts : Facts from cv_facts topology : Container topology","title":"Required Inputs"},{"location":"docs/how-to/cv_container/#optional-inputs","text":"mode : Define how to manage container available on CV and not in customer topology - merge (default) - delete - override - name : lab04 - cv_container lab hosts : CloudVision connection : local gather_facts : no tasks : - name : \"Gather CVP facts {{inventory_hostname}}\" arista.cvp.cv_facts : register : CVP_FACTS - name : \"Configure containers on {{inventory_hostname}}\" arista.cvp.cv_container : cvp_facts : \"{{CVP_FACTS.ansible_facts}}\" topology : \"{{CVP_CONTAINERS}}\"","title":"Optional inputs"},{"location":"docs/how-to/cv_container/#module-output","text":"cv_container returns : List of created containers List of deleted containers List of moved devices List of attached configlets List of CV tasks generated Generated tasks can be consumed directly by cv_tasks. { \"changed\" : false , \"data\" : { \"attached_configlet\" : { \"configlet_attached\" : 4 , \"list\" : [ [ { \"config\" : \"alias v10 show version\" , \"containers\" : [ ], \"devices\" : [ ], \"key\" : \"configlet_885_1325820320363417\" , \"name\" : \"alias\" , \"type\" : \"Static\" } ] ], \"taskIds\" : [ \"127\" ] }, \"changed\" : true , \"creation_result\" : { \"containers_created\" : \"4\" }, \"deletion_result\" : { \"containers_deleted\" : \"1\" }, \"moved_result\" : { \"devices_moved\" : 3 , \"list\" : [ \"veos01\" , \"veos02\" , \"veos03\" ], \"taskIds\" : [ \"125\" , \"126\" , \"127\" ] }, \"tasks\" : [] }, \"failed\" : false }","title":"Module output"},{"location":"docs/how-to/cv_device/","text":"Configure devices on Cloudvision # cv_device manage devices on CloudVision: Support Configlets attachment Support Container move during provisioning Support device reset if required cv_device bases its actions on cv_facts results The cv_device actions are based on cv_facts results: Use intend approach No declarative action Inputs # Full documentation available in module section and a lab is available in the following repository Input variables # Device name. Parent container where to move device. List of configlets to apply to the device. CVP_DEVICES : TEAM01-SPINE1 : name : TEAM01-SPINE1 parent_container : STAGING configlets : - TEAM01-SPINE1 imageBundle : [] # Not yet supported TEAM02-SPINE1 : name : TEAM02-SPINE1 parent_container : STAGING configlets : - TEAM02-SPINE1 imageBundle : [] # Not yet supporte Module inputs # Required Inputs # cvp_facts : Facts from cv_facts devices : List of devices device_filter : Filter to only target devices as defined in list. Optional inputs # state : Define if module should create (default) or delete devices from CV - name : \"Configure devices on {{inventory_hostname}}\" arista.cvp.cv_device : devices : \"{{CVP_DEVICES}}\" cvp_facts : '{{CVP_FACTS.ansible_facts}}' device_filter : [ 'TEAM' ] state : present register : CVP_DEVICES_RESULTS Module output # cv_device returns : Generated tasks can be consumed directly by cv_tasks. { \"msg\" : { \"changed\" : true , \"data\" : { \"new\" : [], \"reset\" : [], \"tasks\" : [ { \"currentTaskName\" : \"Submit\" , \"description\" : \"Ansible Configlet Update: on Device TEAM01-SPINE1\" , \"note\" : \"\" , \"taskStatus\" : \"ACTIVE\" , \"workOrderDetails\" : { \"ipAddress\" : \"10.255.0.11\" , \"netElementHostName\" : \"TEAM01-SPINE1\" , \"netElementId\" : \"0c:1a:c1:ed:98:18\" , \"serialNumber\" : \"6B25F852A3A3036E1ADBB4423F1E62EF\" , \"workOrderDetailsId\" : \"\" , \"workOrderId\" : \"\" }, \"workOrderId\" : \"8\" , \"workOrderState\" : \"ACTIVE\" , \"workOrderUserDefinedStatus\" : \"Pending\" } ], \"updated\" : [ { \"TEAM01-SPINE1\" : \"Configlets-[u'8']\" }, { \"TEAM01-SPINE1\" : \"Device TEAM 01 -SPINE 1 \\ imageBundle cannot be updated - Exception : imageBundle_key \\ check : No imageBundle specified \" } ] }, \" failed\" : false } }","title":"Manage devices"},{"location":"docs/how-to/cv_device/#configure-devices-on-cloudvision","text":"cv_device manage devices on CloudVision: Support Configlets attachment Support Container move during provisioning Support device reset if required cv_device bases its actions on cv_facts results The cv_device actions are based on cv_facts results: Use intend approach No declarative action","title":"Configure devices on Cloudvision"},{"location":"docs/how-to/cv_device/#inputs","text":"Full documentation available in module section and a lab is available in the following repository","title":"Inputs"},{"location":"docs/how-to/cv_device/#input-variables","text":"Device name. Parent container where to move device. List of configlets to apply to the device. CVP_DEVICES : TEAM01-SPINE1 : name : TEAM01-SPINE1 parent_container : STAGING configlets : - TEAM01-SPINE1 imageBundle : [] # Not yet supported TEAM02-SPINE1 : name : TEAM02-SPINE1 parent_container : STAGING configlets : - TEAM02-SPINE1 imageBundle : [] # Not yet supporte","title":"Input variables"},{"location":"docs/how-to/cv_device/#module-inputs","text":"","title":"Module inputs"},{"location":"docs/how-to/cv_device/#required-inputs","text":"cvp_facts : Facts from cv_facts devices : List of devices device_filter : Filter to only target devices as defined in list.","title":"Required Inputs"},{"location":"docs/how-to/cv_device/#optional-inputs","text":"state : Define if module should create (default) or delete devices from CV - name : \"Configure devices on {{inventory_hostname}}\" arista.cvp.cv_device : devices : \"{{CVP_DEVICES}}\" cvp_facts : '{{CVP_FACTS.ansible_facts}}' device_filter : [ 'TEAM' ] state : present register : CVP_DEVICES_RESULTS","title":"Optional inputs"},{"location":"docs/how-to/cv_device/#module-output","text":"cv_device returns : Generated tasks can be consumed directly by cv_tasks. { \"msg\" : { \"changed\" : true , \"data\" : { \"new\" : [], \"reset\" : [], \"tasks\" : [ { \"currentTaskName\" : \"Submit\" , \"description\" : \"Ansible Configlet Update: on Device TEAM01-SPINE1\" , \"note\" : \"\" , \"taskStatus\" : \"ACTIVE\" , \"workOrderDetails\" : { \"ipAddress\" : \"10.255.0.11\" , \"netElementHostName\" : \"TEAM01-SPINE1\" , \"netElementId\" : \"0c:1a:c1:ed:98:18\" , \"serialNumber\" : \"6B25F852A3A3036E1ADBB4423F1E62EF\" , \"workOrderDetailsId\" : \"\" , \"workOrderId\" : \"\" }, \"workOrderId\" : \"8\" , \"workOrderState\" : \"ACTIVE\" , \"workOrderUserDefinedStatus\" : \"Pending\" } ], \"updated\" : [ { \"TEAM01-SPINE1\" : \"Configlets-[u'8']\" }, { \"TEAM01-SPINE1\" : \"Device TEAM 01 -SPINE 1 \\ imageBundle cannot be updated - Exception : imageBundle_key \\ check : No imageBundle specified \" } ] }, \" failed\" : false } }","title":"Module output"},{"location":"docs/how-to/cv_facts/","text":"Get facts from Cloudvision # cv_facts collect Facts from CloudVision: CV version List of devices part of CV. - Active EOS devices - Inactive EOS devices - 3 rd part devices List of configlets List of containers List of tasks Full documentation available in module section and a lab is available in the following repository Playbook example # Standard playbook # --- - name : lab02 - cv_facts lab hosts : CloudVision connection : local gather_facts : no tasks : - name : \"Gather CVP facts {{inventory_hostname}}\" arista.cvp.cv_facts : register : cv_facts Only collect a set of facts # tasks : - name : \"Gather CVP facts {{inventory_hostname}}\" arista.cvp.cv_facts : facts : configlets Collect running-configuration of devices # tasks : - name : \"Gather CVP facts {{inventory_hostname}}\" arista.cvp.cv_facts : facts : devices gather_subset : config Module output # Output is JSON and can be saved or considered as input by other modules { \"ansible_facts\" : { \"cvp_info\" : { \"appVersion\" : \"Foster_Build_03\" , \"version\" : \"2018.2.5\" }, \"configlets\" : [ { \"name\" : \"ANSIBLE_TESTING_CONTAINER\" , \"isDefault\" : \"no\" , \"config\" : \"alias a57 show version\" , \"reconciled\" : false , \"netElementCount\" : 3 , \"editable\" : true , \"dateTimeInLongFormat\" : 1574944821353 , \"isDraft\" : false , \"note\" : \"## Managed by Ansible ##\" , \"visible\" : true , \"containerCount\" : 2 , \"user\" : \"cvpadmin\" , \"key\" : \"configlet_3503_4572477104617871\" , \"sslConfig\" : false , \"devices\" : [ \"veos01\" , \"veos03\" ], [...]","title":"Get Facts"},{"location":"docs/how-to/cv_facts/#get-facts-from-cloudvision","text":"cv_facts collect Facts from CloudVision: CV version List of devices part of CV. - Active EOS devices - Inactive EOS devices - 3 rd part devices List of configlets List of containers List of tasks Full documentation available in module section and a lab is available in the following repository","title":"Get facts from Cloudvision"},{"location":"docs/how-to/cv_facts/#playbook-example","text":"","title":"Playbook example"},{"location":"docs/how-to/cv_facts/#standard-playbook","text":"--- - name : lab02 - cv_facts lab hosts : CloudVision connection : local gather_facts : no tasks : - name : \"Gather CVP facts {{inventory_hostname}}\" arista.cvp.cv_facts : register : cv_facts","title":"Standard playbook"},{"location":"docs/how-to/cv_facts/#only-collect-a-set-of-facts","text":"tasks : - name : \"Gather CVP facts {{inventory_hostname}}\" arista.cvp.cv_facts : facts : configlets","title":"Only collect a set of facts"},{"location":"docs/how-to/cv_facts/#collect-running-configuration-of-devices","text":"tasks : - name : \"Gather CVP facts {{inventory_hostname}}\" arista.cvp.cv_facts : facts : devices gather_subset : config","title":"Collect running-configuration of devices"},{"location":"docs/how-to/cv_facts/#module-output","text":"Output is JSON and can be saved or considered as input by other modules { \"ansible_facts\" : { \"cvp_info\" : { \"appVersion\" : \"Foster_Build_03\" , \"version\" : \"2018.2.5\" }, \"configlets\" : [ { \"name\" : \"ANSIBLE_TESTING_CONTAINER\" , \"isDefault\" : \"no\" , \"config\" : \"alias a57 show version\" , \"reconciled\" : false , \"netElementCount\" : 3 , \"editable\" : true , \"dateTimeInLongFormat\" : 1574944821353 , \"isDraft\" : false , \"note\" : \"## Managed by Ansible ##\" , \"visible\" : true , \"containerCount\" : 2 , \"user\" : \"cvpadmin\" , \"key\" : \"configlet_3503_4572477104617871\" , \"sslConfig\" : false , \"devices\" : [ \"veos01\" , \"veos03\" ], [...]","title":"Module output"},{"location":"docs/how-to/cvp-authentication/","text":"Cloudvision Authentication # Cloudvision supports 2 different types of authentication depending on what kind of instance you are targeting: On-premise Cloudvision instance: username and password authentication Cloudvision-as-a-Service : User token authentication On-premise Cloudvision authentication # This authentication mechanism is default approach leveraged in the collection and can be configured as below in your variables. It is based on a pure username/password model # Default Ansible variables for authentication ansible_host : < IP address or hostname to target > ansible_user : < Username to connect to CVP instance > ansible_ssh_pass : < Password to use to connect to CVP instance > ansible_connection : httpapi ansible_network_os : eos # Optional Ansible become configuration. ansible_become : true ansible_become_method : enable # HTTPAPI plugin configuration ansible_httpapi_port : '{{ansible_port}}' ansible_httpapi_host : '{{ ansible_host }}' ansible_httpapi_use_ssl : true ansible_httpapi_validate_certs : false Cloudvision as a Service authentication # This authentication method leverage a user token to first get from your CVaaS instance. Then, instruct ansible to use token instead of username and password authentication # Default Ansible variables for authentication ansible_host : < IP address or hostname to target > ansible_user : cvaas # Shall not be changed. ansible will switch to cvaas mode ansible_ssh_pass : < User token to use to connect to CVP instance > ansible_connection : httpapi ansible_network_os : eos # Optional Ansible become configuration. ansible_become : true ansible_become_method : enable # HTTPAPI plugin configuration ansible_httpapi_port : '{{ansible_port}}' ansible_httpapi_host : '{{ ansible_host }}' ansible_httpapi_use_ssl : true ansible_httpapi_validate_certs : false How to validate SSL certificate # Validate SSL cert signed by public CA # Starting version 2.1.1 , arista.cvp collection supports mechanism to validate SSL certificate. To configure ansible to validate SSL certificate provided by your CV instance, you must update httpapi information like this: # HTTPAPI plugin configuration ansible_httpapi_port : '{{ansible_port}}' ansible_httpapi_host : '{{ ansible_host }}' ansible_httpapi_use_ssl : true ansible_httpapi_validate_certs : true Validate SSL cert signed by unknown CA # This mechanism works also with self-signed certificate Update httpapi as shown below: # HTTPAPI plugin configuration ansible_httpapi_port : '{{ansible_port}}' ansible_httpapi_host : '{{ ansible_host }}' ansible_httpapi_use_ssl : true ansible_httpapi_validate_certs : true And then, import your CA or server CRT file into database of your CA for Python # Get CVP SSL Cert (If not already provided by your CV admin) $ true | openssl s_client -connect <YOUR-CV-IP>:443 2 >/dev/null | openssl x509 > cvp.crt # Update Python DB for known CA $ cat cvp.crt >> ` python -m certifi ` Note it is per virtual environment configuration. Invalid SSL certification # If identity cannot be validated by ansible, playbook stops with following error message: $ ansible-playbook playbooks/extract-facts.yml PLAY [ CV Facts ] *************************************************************** TASK [ Gather CVP facts from cv_server ] **************************************** Monday 05 October 2020 21 :09:22 +0200 ( 0 :00:00.063 ) 0 :00:00.063 ******** Monday 05 October 2020 21 :09:22 +0200 ( 0 :00:00.063 ) 0 :00:00.063 ******** fatal: [ cv_server ] : FAILED! = > changed = false msg: | 2 - x.x.x.x: HTTPSConnectionPool ( host = 'x.x.x.x' , port = 443 ) : Max retries \\ exceeded with url: /web/login/authenticate.do \\ ( Caused by SSLError ( SSLError ( 1 , '[SSL: CERTIFICATE_VERIFY_FAILED] \\ certificate verify failed (_ssl.c:852)' ) , ))","title":"Configure Cloudvision Authentication"},{"location":"docs/how-to/cvp-authentication/#cloudvision-authentication","text":"Cloudvision supports 2 different types of authentication depending on what kind of instance you are targeting: On-premise Cloudvision instance: username and password authentication Cloudvision-as-a-Service : User token authentication","title":"Cloudvision Authentication"},{"location":"docs/how-to/cvp-authentication/#on-premise-cloudvision-authentication","text":"This authentication mechanism is default approach leveraged in the collection and can be configured as below in your variables. It is based on a pure username/password model # Default Ansible variables for authentication ansible_host : < IP address or hostname to target > ansible_user : < Username to connect to CVP instance > ansible_ssh_pass : < Password to use to connect to CVP instance > ansible_connection : httpapi ansible_network_os : eos # Optional Ansible become configuration. ansible_become : true ansible_become_method : enable # HTTPAPI plugin configuration ansible_httpapi_port : '{{ansible_port}}' ansible_httpapi_host : '{{ ansible_host }}' ansible_httpapi_use_ssl : true ansible_httpapi_validate_certs : false","title":"On-premise Cloudvision authentication"},{"location":"docs/how-to/cvp-authentication/#cloudvision-as-a-service-authentication","text":"This authentication method leverage a user token to first get from your CVaaS instance. Then, instruct ansible to use token instead of username and password authentication # Default Ansible variables for authentication ansible_host : < IP address or hostname to target > ansible_user : cvaas # Shall not be changed. ansible will switch to cvaas mode ansible_ssh_pass : < User token to use to connect to CVP instance > ansible_connection : httpapi ansible_network_os : eos # Optional Ansible become configuration. ansible_become : true ansible_become_method : enable # HTTPAPI plugin configuration ansible_httpapi_port : '{{ansible_port}}' ansible_httpapi_host : '{{ ansible_host }}' ansible_httpapi_use_ssl : true ansible_httpapi_validate_certs : false","title":"Cloudvision as a Service authentication"},{"location":"docs/how-to/cvp-authentication/#how-to-validate-ssl-certificate","text":"","title":"How to validate SSL certificate"},{"location":"docs/how-to/cvp-authentication/#validate-ssl-cert-signed-by-public-ca","text":"Starting version 2.1.1 , arista.cvp collection supports mechanism to validate SSL certificate. To configure ansible to validate SSL certificate provided by your CV instance, you must update httpapi information like this: # HTTPAPI plugin configuration ansible_httpapi_port : '{{ansible_port}}' ansible_httpapi_host : '{{ ansible_host }}' ansible_httpapi_use_ssl : true ansible_httpapi_validate_certs : true","title":"Validate SSL cert signed by public CA"},{"location":"docs/how-to/cvp-authentication/#validate-ssl-cert-signed-by-unknown-ca","text":"This mechanism works also with self-signed certificate Update httpapi as shown below: # HTTPAPI plugin configuration ansible_httpapi_port : '{{ansible_port}}' ansible_httpapi_host : '{{ ansible_host }}' ansible_httpapi_use_ssl : true ansible_httpapi_validate_certs : true And then, import your CA or server CRT file into database of your CA for Python # Get CVP SSL Cert (If not already provided by your CV admin) $ true | openssl s_client -connect <YOUR-CV-IP>:443 2 >/dev/null | openssl x509 > cvp.crt # Update Python DB for known CA $ cat cvp.crt >> ` python -m certifi ` Note it is per virtual environment configuration.","title":"Validate SSL cert signed by unknown CA"},{"location":"docs/how-to/cvp-authentication/#invalid-ssl-certification","text":"If identity cannot be validated by ansible, playbook stops with following error message: $ ansible-playbook playbooks/extract-facts.yml PLAY [ CV Facts ] *************************************************************** TASK [ Gather CVP facts from cv_server ] **************************************** Monday 05 October 2020 21 :09:22 +0200 ( 0 :00:00.063 ) 0 :00:00.063 ******** Monday 05 October 2020 21 :09:22 +0200 ( 0 :00:00.063 ) 0 :00:00.063 ******** fatal: [ cv_server ] : FAILED! = > changed = false msg: | 2 - x.x.x.x: HTTPSConnectionPool ( host = 'x.x.x.x' , port = 443 ) : Max retries \\ exceeded with url: /web/login/authenticate.do \\ ( Caused by SSLError ( SSLError ( 1 , '[SSL: CERTIFICATE_VERIFY_FAILED] \\ certificate verify failed (_ssl.c:852)' ) , ))","title":"Invalid SSL certification"},{"location":"docs/installation/development/","text":"Development Tips & Tricks # Overview # Two methods can be used get Ansible up and running quickly with all the requirements to leverage ansible-cvp. A Python Virtual Environment or Docker container. The best way to use the development files, is to copy them to the root directory where you have your repositories cloned. For example, see the file/folder structure below. \u251c\u2500\u2500 git_projects \u2502 \u251c\u2500\u2500 ansible-avd \u2502 \u251c\u2500\u2500 ansible-cvp \u2502 \u251c\u2500\u2500 netdevops-examples | \u251c\u2500\u2500 <YOUR OWN TESTING REPOSITORY> \u2502 \u251c\u2500\u2500 Makefile Build local environment # Please refer to Setup environment page Once installed, use dev-start command to bring up all the required containers: An mkdoc for AVD documentation listening on port localhost:8000 An mkdoc or CVP documentation listening on port localhost:8001 An AVD runner with a pseudo terminal connected to shell for ansible execution Docker things # he docker container approach for development can be used to ensure that everybody is using the same development environment while still being flexible enough to use the repo you are making changes in. You can inspect the Dockerfile to see what packages have been installed. The container will mount the current working directory, so you can work with your local files. The ansible version is passed in with the docker build command using ANSIBLE_VERSION variable. If the ANSIBLE variable is not used the Dockerfile will by default set the ansible version to describe in AVD requirements. Before you can use a container, you must install Docker CE and docker-compose on your workstation. Since docker image is now automatically published on docker-hub , a dedicated repository is available on Arista Netdevops Community . # Start development stack $ make dev-start docker-compose -f ansible-cvp/development/docker-compose.yml up -d Recreating development_ansible_1 ... done Recreating development_webdoc_cvp_1 ... done Recreating development_webdoc_avd_1 ... done # List containers started with stack $ docker-compose -f ansible-cvp/development/docker-compose.yml ps Name Command State Ports ----------------------------------------------------------------------------- ansible_avd /bin/sh -c while true ; do ... Up webdoc_avd sh -c pip install -r ansib ... Up 0 .0.0.0:8000->8000/tcp webdoc_cvp sh -c pip install -r ansib ... Up 0 .0.0.0:8001->8000/tcp # Get a shell with ansible (if not in shell from previous command) $ make dev-run docker-compose -f ansible-cvp/development/docker-compose.yml exec ansible zsh Agent pid 52 \u279c /projects # Test MKDOCS access (outside of development container) $ curl -s http://127.0.0.1:8001 | head -n 10 <!doctype html> <html lang = \"en\" class = \"no-js\" > <head> <meta charset = \"utf-8\" > <meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > # Stop development stack $ make dev-stop docker-compose -f ansible-cvp/development/docker-compose.yml kill && \\ docker-compose -f ansible-cvp/development/docker-compose.yml rm -f Killing development_ansible_1 ... done Killing development_webdoc_1 ... done Going to remove development_ansible_1, development_webdoc_1 Removing development_ansible_1 ... done Removing development_webdoc_1 ... done Development tools # Pre-commit hook # pre-commit can run standard hooks on every commit to automatically point out issues in code such as missing semicolons, trailing whitespace, and debug statements. By pointing these issues out before code review, this allows a code reviewer to focus on the architecture of a change while not wasting time with trivial style nitpicks. Repository implements following hooks: trailing-whitespace : Fix trailing whitespace. if found, commit is stopped and you must run commit process again. end-of-file-fixer : Like trailing-whitespace , this hook fix wrong end of file and stop your commit. check-yaml : Check all YAML files ares valid check-added-large-files : Check if there is no large file included in repository check-merge-conflict : Validate there is no MERGE syntax related to a invalid merge process. pylint : Run python linting with settings defined in pylintrc yamllint : Validate all YAML files using configuration from yamllintrc ansible-lint : Validate yaml files are valid against ansible rules. Installation # pre-commit is part of development requirememnts . To install, run pip command in ansible-avd folder: $ pip install -r development/requirements-dev.txt ... Run pre-commit manually # To run pre-commit manually before your commit, use this command: pre-commit run [ WARNING ] Unstaged files detected. [ INFO ] Stashing unstaged files to /Users/xxx/.cache/pre-commit/patch1590742434. Trim Trailing Whitespace............................. ( no files to check ) Skipped Fix End of Files..................................... ( no files to check ) Skipped Check Yaml........................................... ( no files to check ) Skipped Check for added large files.......................... ( no files to check ) Skipped Check for merge conflicts............................ ( no files to check ) Skipped Check for Linting error on Python files.............. ( no files to check ) Skipped Check for Linting error on YAML files................ ( no files to check ) Skipped Check for ansible-lint errors............................................Passed [ INFO ] Restored changes from /Users/xxx/.cache/pre-commit/patch1590742434. Command will automatically detect changed files using git status and run tests according their type. This process is also implemented in project CI to ensure code quality and compliance with ansible development process. Configure git hook # To automatically run tests when running a commit, configure your repository whit command: $ pre-commit install pre-commit installed at .git/hooks/pre-commit To remove installation, use uninstall option. Check 404 links # To validate documentation, you should check for not found links in your local version of the documentation. This test requires to run mkdocs container as explained in installation documentation . In a shell, run the following make command. It starts a container in CVP documentation network and leverage muffet tool to check 404 HTTP code: $ check-cvp-404 docker run --network container:webdoc_cvp raviqqe/muffet \\ http://127.0.0.1:8001 \\ -e \".*fonts.gstatic.com.*\" \\ -e \".*edit.*\" \\ -f --limit-redirections = 3 \\ --timeout = 60 http://127.0.0.1:8001/docs/installation/development/ 404 http://127.0.0.1:8001/docs/installation/development/setup-environement2.md make: *** [ check-cvp-404 ] Error 1 This process is also implemented in project CI to protect documentation against dead links.","title":"Development tips & tricks"},{"location":"docs/installation/development/#development-tips-tricks","text":"","title":"Development Tips &amp; Tricks"},{"location":"docs/installation/development/#overview","text":"Two methods can be used get Ansible up and running quickly with all the requirements to leverage ansible-cvp. A Python Virtual Environment or Docker container. The best way to use the development files, is to copy them to the root directory where you have your repositories cloned. For example, see the file/folder structure below. \u251c\u2500\u2500 git_projects \u2502 \u251c\u2500\u2500 ansible-avd \u2502 \u251c\u2500\u2500 ansible-cvp \u2502 \u251c\u2500\u2500 netdevops-examples | \u251c\u2500\u2500 <YOUR OWN TESTING REPOSITORY> \u2502 \u251c\u2500\u2500 Makefile","title":"Overview"},{"location":"docs/installation/development/#build-local-environment","text":"Please refer to Setup environment page Once installed, use dev-start command to bring up all the required containers: An mkdoc for AVD documentation listening on port localhost:8000 An mkdoc or CVP documentation listening on port localhost:8001 An AVD runner with a pseudo terminal connected to shell for ansible execution","title":"Build local environment"},{"location":"docs/installation/development/#docker-things","text":"he docker container approach for development can be used to ensure that everybody is using the same development environment while still being flexible enough to use the repo you are making changes in. You can inspect the Dockerfile to see what packages have been installed. The container will mount the current working directory, so you can work with your local files. The ansible version is passed in with the docker build command using ANSIBLE_VERSION variable. If the ANSIBLE variable is not used the Dockerfile will by default set the ansible version to describe in AVD requirements. Before you can use a container, you must install Docker CE and docker-compose on your workstation. Since docker image is now automatically published on docker-hub , a dedicated repository is available on Arista Netdevops Community . # Start development stack $ make dev-start docker-compose -f ansible-cvp/development/docker-compose.yml up -d Recreating development_ansible_1 ... done Recreating development_webdoc_cvp_1 ... done Recreating development_webdoc_avd_1 ... done # List containers started with stack $ docker-compose -f ansible-cvp/development/docker-compose.yml ps Name Command State Ports ----------------------------------------------------------------------------- ansible_avd /bin/sh -c while true ; do ... Up webdoc_avd sh -c pip install -r ansib ... Up 0 .0.0.0:8000->8000/tcp webdoc_cvp sh -c pip install -r ansib ... Up 0 .0.0.0:8001->8000/tcp # Get a shell with ansible (if not in shell from previous command) $ make dev-run docker-compose -f ansible-cvp/development/docker-compose.yml exec ansible zsh Agent pid 52 \u279c /projects # Test MKDOCS access (outside of development container) $ curl -s http://127.0.0.1:8001 | head -n 10 <!doctype html> <html lang = \"en\" class = \"no-js\" > <head> <meta charset = \"utf-8\" > <meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > # Stop development stack $ make dev-stop docker-compose -f ansible-cvp/development/docker-compose.yml kill && \\ docker-compose -f ansible-cvp/development/docker-compose.yml rm -f Killing development_ansible_1 ... done Killing development_webdoc_1 ... done Going to remove development_ansible_1, development_webdoc_1 Removing development_ansible_1 ... done Removing development_webdoc_1 ... done","title":"Docker things"},{"location":"docs/installation/development/#development-tools","text":"","title":"Development tools"},{"location":"docs/installation/development/#pre-commit-hook","text":"pre-commit can run standard hooks on every commit to automatically point out issues in code such as missing semicolons, trailing whitespace, and debug statements. By pointing these issues out before code review, this allows a code reviewer to focus on the architecture of a change while not wasting time with trivial style nitpicks. Repository implements following hooks: trailing-whitespace : Fix trailing whitespace. if found, commit is stopped and you must run commit process again. end-of-file-fixer : Like trailing-whitespace , this hook fix wrong end of file and stop your commit. check-yaml : Check all YAML files ares valid check-added-large-files : Check if there is no large file included in repository check-merge-conflict : Validate there is no MERGE syntax related to a invalid merge process. pylint : Run python linting with settings defined in pylintrc yamllint : Validate all YAML files using configuration from yamllintrc ansible-lint : Validate yaml files are valid against ansible rules.","title":"Pre-commit hook"},{"location":"docs/installation/development/#installation","text":"pre-commit is part of development requirememnts . To install, run pip command in ansible-avd folder: $ pip install -r development/requirements-dev.txt ...","title":"Installation"},{"location":"docs/installation/development/#run-pre-commit-manually","text":"To run pre-commit manually before your commit, use this command: pre-commit run [ WARNING ] Unstaged files detected. [ INFO ] Stashing unstaged files to /Users/xxx/.cache/pre-commit/patch1590742434. Trim Trailing Whitespace............................. ( no files to check ) Skipped Fix End of Files..................................... ( no files to check ) Skipped Check Yaml........................................... ( no files to check ) Skipped Check for added large files.......................... ( no files to check ) Skipped Check for merge conflicts............................ ( no files to check ) Skipped Check for Linting error on Python files.............. ( no files to check ) Skipped Check for Linting error on YAML files................ ( no files to check ) Skipped Check for ansible-lint errors............................................Passed [ INFO ] Restored changes from /Users/xxx/.cache/pre-commit/patch1590742434. Command will automatically detect changed files using git status and run tests according their type. This process is also implemented in project CI to ensure code quality and compliance with ansible development process.","title":"Run pre-commit manually"},{"location":"docs/installation/development/#configure-git-hook","text":"To automatically run tests when running a commit, configure your repository whit command: $ pre-commit install pre-commit installed at .git/hooks/pre-commit To remove installation, use uninstall option.","title":"Configure git hook"},{"location":"docs/installation/development/#check-404-links","text":"To validate documentation, you should check for not found links in your local version of the documentation. This test requires to run mkdocs container as explained in installation documentation . In a shell, run the following make command. It starts a container in CVP documentation network and leverage muffet tool to check 404 HTTP code: $ check-cvp-404 docker run --network container:webdoc_cvp raviqqe/muffet \\ http://127.0.0.1:8001 \\ -e \".*fonts.gstatic.com.*\" \\ -e \".*edit.*\" \\ -f --limit-redirections = 3 \\ --timeout = 60 http://127.0.0.1:8001/docs/installation/development/ 404 http://127.0.0.1:8001/docs/installation/development/setup-environement2.md make: *** [ check-cvp-404 ] Error 1 This process is also implemented in project CI to protect documentation against dead links.","title":"Check 404 links"},{"location":"docs/installation/requirements/","text":"Requirements # Arista EOS version # EOS 4.21.8M or later Roles validated with eAPI transport -> ansible_connection: httpapi Arista Cloudvision # Cloudvision instance must be supported by Cloudvision ansible collection Python # Python 3.6.8 or later Supported Ansible Versions # ansible 2.9.2 or later previous ansible version not supported as avd is shipped as an ansible collection Additional Python Libraries required # Jinja2 2.10.3 netaddr 0.7.19 requests 2.22.0 treelib 1.5.5 cvprac 1.0.4 Python requirements installation # In a shell, run following command: $ pip3 install -r development/requirements.txt requirements.txt has the following content: ansible==2.9.6 netaddr==0.7.19 Jinja2==2.10.3 requests==2.22.0 treelib==1.5.5 cvprac==1.0.4 Depending of your operating system settings, pip3 might be replaced by pip . Ansible runner requirements # A optional docker container is available with all the requirements already installed. To use this container, Docker must be installed on your ansible runner. To install Docker on your system, you can refer to the following page: Docker installation step by step Or if you prefer you can run this oneLiner installation script: $ curl -fsSL get.docker.com | sh In addition, docker-compose should be considered to run a stack of containers: https://docs.docker.com/compose/install/","title":"Requirements"},{"location":"docs/installation/requirements/#requirements","text":"","title":"Requirements"},{"location":"docs/installation/requirements/#arista-eos-version","text":"EOS 4.21.8M or later Roles validated with eAPI transport -> ansible_connection: httpapi","title":"Arista EOS version"},{"location":"docs/installation/requirements/#arista-cloudvision","text":"Cloudvision instance must be supported by Cloudvision ansible collection","title":"Arista Cloudvision"},{"location":"docs/installation/requirements/#python","text":"Python 3.6.8 or later","title":"Python"},{"location":"docs/installation/requirements/#supported-ansible-versions","text":"ansible 2.9.2 or later previous ansible version not supported as avd is shipped as an ansible collection","title":"Supported Ansible Versions"},{"location":"docs/installation/requirements/#additional-python-libraries-required","text":"Jinja2 2.10.3 netaddr 0.7.19 requests 2.22.0 treelib 1.5.5 cvprac 1.0.4","title":"Additional Python Libraries required"},{"location":"docs/installation/requirements/#python-requirements-installation","text":"In a shell, run following command: $ pip3 install -r development/requirements.txt requirements.txt has the following content: ansible==2.9.6 netaddr==0.7.19 Jinja2==2.10.3 requests==2.22.0 treelib==1.5.5 cvprac==1.0.4 Depending of your operating system settings, pip3 might be replaced by pip .","title":"Python requirements installation"},{"location":"docs/installation/requirements/#ansible-runner-requirements","text":"A optional docker container is available with all the requirements already installed. To use this container, Docker must be installed on your ansible runner. To install Docker on your system, you can refer to the following page: Docker installation step by step Or if you prefer you can run this oneLiner installation script: $ curl -fsSL get.docker.com | sh In addition, docker-compose should be considered to run a stack of containers: https://docs.docker.com/compose/install/","title":"Ansible runner requirements"},{"location":"docs/installation/setup-docker/","text":"Docker & Development environment # Two methods can be used get Ansible up and running quickly with all the requirements to leverage ansible-avd. A Python Virtual Environment or Docker container. The best way to use the development files, is to copy them to the root directory where you have your repositories cloned. For example, see the file/folder structure below. \u251c\u2500\u2500 git_projects \u2502 \u251c\u2500\u2500 ansible-avd \u2502 \u251c\u2500\u2500 ansible-cvp \u2502 \u251c\u2500\u2500 ansible-avd-cloudvision-demo \u2502 \u251c\u2500\u2500 <your-own-test-folder> \u2502 \u251c\u2500\u2500 Makefile ... Step by step installation process # This process is similar to ansible-avd collection . It means if you use AVD, this step is not required. mkdir git_projects cd git_projects git clone https://github.com/aristanetworks/ansible-avd.git git clone https://github.com/aristanetworks/ansible-cvp.git git clone https://github.com/arista-netdevops-community/ansible-avd-cloudvision-demo.git cp ansible-cvp/development/Makefile ./ make run One liner installation # One liner script to setup a development environment. it does following actions: Create local folder for development Instantiate a local git repository (no remote) Clone AVD and CVP collections Deploy Makefile $ sh -c \" $( curl -fsSL https://get.avd.sh ) \" Build local environment # Docker Container for Ansible Testing and Development # The docker container approach for development can be used to ensure that everybody is using the same development environment while still being flexible enough to use the repo you are making changes in. You can inspect the Dockerfile to see what packages have been installed. The container will mount the current working directory, so you can work with your local files. The ansible version is passed in with the docker build command using ANSIBLE variable. If the ANSIBLE variable is not used the Dockerfile will by default set the ansible version to 2.9.2 Before you can use a container, you must install Docker CE and docker-compose on your workstation. Development containers # Ansible shell : provide a built-in container with all AVD and CVP requirements already installed. MKDOCS for documentation update: Run MKDOCS in a container and expose port 8000 to test and validate markdown rendering for AVD site. Container commands # In this folder you have a Makefile providing a list of commands to start a development environment: run : Start a shell within a container and local folder mounted in /projects dev-start : Start a stack of containers based on docker-compose: 1 container for ansible playbooks and 1 container for mkdocs dev-stop : Stop compose stack and remove containers. dev-run : Connect to ansible container to run your test playbooks. dev-reload : Run stop and start. If you want to test a specific ansible version, you can refer to this dedicated page to start your own docker image. You can also use following make command: make ANSIBLE_VERSION=2.9.3 run Since docker image is now automatically published on docker-hub , a dedicated repository is available on Arista Netdevops Community . # Start development stack $ make dev-start docker-compose -f ansible-cvp/development/docker-compose.yml up -d Creating development_webdoc_1 ... done Creating development_ansible_1 ... done # List containers started with stack $ docker-compose -f ansible-avd/development/docker-compose.yml ps Name Command State Ports --------------------------------------------------------------------------------------- development_ansible_1 /bin/sh -c while true ; do ... Up development_webdoc_1 sh -c pip install -r ansib ... Up 0 .0.0.0:8000->8000/tcp # Get a shell with ansible $ make dev-run docker-compose -f ansible-cvp/development/docker-compose.yml exec ansible zsh Agent pid 52 \u279c /projects # Test MKDOCS access $ curl -s http://127.0.0.1:8001 | head -n 10 <!doctype html> <html lang = \"en\" class = \"no-js\" > <head> <meta charset = \"utf-8\" > <meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > # Stop development stack $ make dev-stop docker-compose -f ansible-cvp/development/docker-compose.yml kill && \\ docker-compose -f ansible-cvp/development/docker-compose.yml rm -f Killing development_ansible_1 ... done Killing development_webdoc_1 ... done Going to remove development_ansible_1, development_webdoc_1 Removing development_ansible_1 ... done Removing development_webdoc_1 ... done","title":"Docker & Development"},{"location":"docs/installation/setup-docker/#docker-development-environment","text":"Two methods can be used get Ansible up and running quickly with all the requirements to leverage ansible-avd. A Python Virtual Environment or Docker container. The best way to use the development files, is to copy them to the root directory where you have your repositories cloned. For example, see the file/folder structure below. \u251c\u2500\u2500 git_projects \u2502 \u251c\u2500\u2500 ansible-avd \u2502 \u251c\u2500\u2500 ansible-cvp \u2502 \u251c\u2500\u2500 ansible-avd-cloudvision-demo \u2502 \u251c\u2500\u2500 <your-own-test-folder> \u2502 \u251c\u2500\u2500 Makefile ...","title":"Docker &amp; Development environment"},{"location":"docs/installation/setup-docker/#step-by-step-installation-process","text":"This process is similar to ansible-avd collection . It means if you use AVD, this step is not required. mkdir git_projects cd git_projects git clone https://github.com/aristanetworks/ansible-avd.git git clone https://github.com/aristanetworks/ansible-cvp.git git clone https://github.com/arista-netdevops-community/ansible-avd-cloudvision-demo.git cp ansible-cvp/development/Makefile ./ make run","title":"Step by step installation process"},{"location":"docs/installation/setup-docker/#one-liner-installation","text":"One liner script to setup a development environment. it does following actions: Create local folder for development Instantiate a local git repository (no remote) Clone AVD and CVP collections Deploy Makefile $ sh -c \" $( curl -fsSL https://get.avd.sh ) \"","title":"One liner installation"},{"location":"docs/installation/setup-docker/#build-local-environment","text":"","title":"Build local environment"},{"location":"docs/installation/setup-docker/#docker-container-for-ansible-testing-and-development","text":"The docker container approach for development can be used to ensure that everybody is using the same development environment while still being flexible enough to use the repo you are making changes in. You can inspect the Dockerfile to see what packages have been installed. The container will mount the current working directory, so you can work with your local files. The ansible version is passed in with the docker build command using ANSIBLE variable. If the ANSIBLE variable is not used the Dockerfile will by default set the ansible version to 2.9.2 Before you can use a container, you must install Docker CE and docker-compose on your workstation.","title":"Docker Container for Ansible Testing and Development"},{"location":"docs/installation/setup-docker/#development-containers","text":"Ansible shell : provide a built-in container with all AVD and CVP requirements already installed. MKDOCS for documentation update: Run MKDOCS in a container and expose port 8000 to test and validate markdown rendering for AVD site.","title":"Development containers"},{"location":"docs/installation/setup-docker/#container-commands","text":"In this folder you have a Makefile providing a list of commands to start a development environment: run : Start a shell within a container and local folder mounted in /projects dev-start : Start a stack of containers based on docker-compose: 1 container for ansible playbooks and 1 container for mkdocs dev-stop : Stop compose stack and remove containers. dev-run : Connect to ansible container to run your test playbooks. dev-reload : Run stop and start. If you want to test a specific ansible version, you can refer to this dedicated page to start your own docker image. You can also use following make command: make ANSIBLE_VERSION=2.9.3 run Since docker image is now automatically published on docker-hub , a dedicated repository is available on Arista Netdevops Community . # Start development stack $ make dev-start docker-compose -f ansible-cvp/development/docker-compose.yml up -d Creating development_webdoc_1 ... done Creating development_ansible_1 ... done # List containers started with stack $ docker-compose -f ansible-avd/development/docker-compose.yml ps Name Command State Ports --------------------------------------------------------------------------------------- development_ansible_1 /bin/sh -c while true ; do ... Up development_webdoc_1 sh -c pip install -r ansib ... Up 0 .0.0.0:8000->8000/tcp # Get a shell with ansible $ make dev-run docker-compose -f ansible-cvp/development/docker-compose.yml exec ansible zsh Agent pid 52 \u279c /projects # Test MKDOCS access $ curl -s http://127.0.0.1:8001 | head -n 10 <!doctype html> <html lang = \"en\" class = \"no-js\" > <head> <meta charset = \"utf-8\" > <meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > # Stop development stack $ make dev-stop docker-compose -f ansible-cvp/development/docker-compose.yml kill && \\ docker-compose -f ansible-cvp/development/docker-compose.yml rm -f Killing development_ansible_1 ... done Killing development_webdoc_1 ... done Going to remove development_ansible_1, development_webdoc_1 Removing development_ansible_1 ... done Removing development_webdoc_1 ... done","title":"Container commands"},{"location":"docs/installation/setup-environement/","text":"Setup Ansible AVD environment # Two methods can be used get Ansible up and running quickly with all the requirements to leverage ansible-avd : A Python Virtual Environment or Docker container . In both scenario, this document will leverage git approach to create a local environment with collections installed in their respective folders and additional folders for all your content. It means, all examples will be based on the following folder structure: \u251c\u2500\u2500 git_projects \u2502 \u251c\u2500\u2500 ansible-avd \u2502 \u251c\u2500\u2500 ansible-cvp \u2502 \u251c\u2500\u2500 ansible-avd-cloudvision-demo \u2502 \u251c\u2500\u2500 Makefile Ansible runner requirements # As described in requirement page , your runner should run Python 3.6.8 or Docker engine with docker-compose . Create local folder structure # To build local folder structure you manually run all the following commands to git clone ansible-avd , ansible-cvp collection and a repository with demo content In addition to this 3 git clone , you can also deployed a Makefile built to provide some shortcut we will discuss in a second stage. $ mkdir git_projects $ cd git_projects $ git clone https://github.com/aristanetworks/ansible-avd.git $ git clone https://github.com/aristanetworks/ansible-cvp.git $ git clone https://github.com/arista-netdevops-community/ansible-avd-cloudvision-demo.git # Copy Makefile at the root position $ cp ansible-avd/development/Makefile ./ $ make shell Or you can use a one-liner script available in ansible-avd repository to create this structure for you. This script does following actions: Create local folder for development Instantiate a local git repository (no remote) Clone AVD and CVP collections Deploy Makefile $ sh -c \" $( curl -fsSL https://get.avd.sh ) \" Because we are cloning ansible collection using git, it is recommended to read documentation about how to setup ansible to use collection based on git clone . Use docker as AVD shell # In this approach Docker container will be leveraged to provides all the AVD requirements and playbooks and collection will be shared from your localhost to the container. This approach make the run process easier as all libraries are pre-configured in container and you can continue to use your preferred text editor to edit and build your content. Considering you have deployed Makefile described in previous section, all the outputs will provide native docker command and the Make command. AVD environment commands # When using installation script to create your own AVD environment, a Makefile is deployed under ./ansible-arista to automate some common commands: $ make <your command> Commands for docker-compose # dev-start : Start docker compose stack to develop with AVD and CVP collection (alias: start ) Deploy an mkdoc instance to expose AVD documentation with live reload for development purposes. Deploy an mkdoc instance to expose CVP documentation with live reload for development purposes. Deploy an AVD runner with a pseudo terminal connected to shell for ansible execution dev-stop : Stop docker compose stack and remove containers (alias: stop ) dev-run : Run a shell attached to ansible container (alias: shell ) dev-reload : Stop and Start docker-compose stack Commands for docker only # run : Run a docker container with local folder mounted under /projects . This command supports some option to test development version like: - ANSIBLE_VERSION : Specific version of ansible to install during container startup. - PIP_REQ : Specific pip requirements file to install during container startup. Command for image management # update : Get latest version of AVD runner and mkdoc servers clean : Remove avd image from local repository Run AVD shell # We are going to start a new container running ansible with all the python requirements and mount local folder under /projects . if image is missing, docker will pull out image for you automatically. $ docker run --rm -it -v $( PWD ) /:/projects avdteam/base:3.6 Unable to find image 'avdteam/base:3.6' locally 3 .6: Pulling from avdteam/base bf5952930446: Already exists 385bb58d08e6: Already exists f59c6df69726: Already exists cc14d0cfa632: Already exists f4eba3bd5be8: Already exists 55c6a5feb373: Already exists 83464a988ea4: Pull complete 9b675b85887d: Pull complete 9cce9aa068f4: Pull complete a49dbba0fea8: Pull complete 793f98fe2265: Pull complete Digest: sha256:ead3ef030caa6caeafd6ddbfd31ce935da26b66914096c9543d9a44cca993dfd Status: Downloaded newer image for avdteam/base:3.6 Agent pid 45 \u279c /projects You can use a Make command to run exact same set of actions: $ make run Unable to find image 'avdteam/base:3.6' locally 3 .6: Pulling from avdteam/base bf5952930446: Already exists 385bb58d08e6: Already exists f59c6df69726: Already exists cc14d0cfa632: Already exists f4eba3bd5be8: Already exists 55c6a5feb373: Already exists 83464a988ea4: Pull complete 9b675b85887d: Pull complete 9cce9aa068f4: Pull complete a49dbba0fea8: Pull complete 793f98fe2265: Pull complete Digest: sha256:ead3ef030caa6caeafd6ddbfd31ce935da26b66914096c9543d9a44cca993dfd Status: Downloaded newer image for avdteam/base:3.6 Agent pid 45 \u279c /projects Then you can move to your content folder as structure remains the same: \u279c /projects ls -l drwxr-xr-x 24 root root 768 Sep 4 15 :47 ansible-avd drwxr-xr-x 24 root root 768 Sep 4 15 :47 ansible-cvp drwxr-xr-x 24 root root 768 Sep 4 15 :47 ansible-avd-cloudvision-demo drwxr-xr-x 24 root root 768 Sep 4 15 :47 Makefile You can validate everything is setup correctly: \u279c /projects python --version Python 3 .6.12 \u279c /projects ansible --version ansible 2 .9.6 config file = None configured module search path = [ '/root/.ansible/plugins/modules' , '/usr/share/ansible/plugins/modules' ] ansible python module location = /root/.local/lib/python3.6/site-packages/ansible executable location = /root/.local/bin/ansible python version = 3 .6.12 ( default, Aug 18 2020 , 04 :28:43 ) [ GCC 8 .3.0 ] To exit container, just use exit \u279c /projects exit $ Get latest image of AVD container # Time to time, AVD container is updated to reflect some changes in either python requirements or ansible version. Because your docker engine won\u2019t automatically get latest version, it might be important to update manually this container: $ docker pull avdteam/base:3.6 latest: Pulling from avdteam/base 8a29a15cefae: Already exists 95df01e08bce: Downloading [============================================== > ] 33 .55MB/36.35MB 512a8a4d71f7: Downloading [========================================= > ] 45 .1MB/53.85MB 209c1657264b: Download complete bd6eece0221e: Downloading [=================== > ] 52 .04MB/132.1MB 036c486feecb: Waiting Your environment is now ready and you can start to build your own project leveraging ansible-avd and ansible-cvp collections. Using Python 3 Virtual Environment feature # This section describes how to configure python to run ansible and AVD. As a requirement, we consider python3 as default python interpreter and pip3 as package manager for python3. Some differences can be spotted depending on your own operating system and how they package python. Disclaimer : Not preferred method. if you are not an experienced user, please use docker approach. In a shell, install virtualenv package: # install virtualenv via pip3 $ sudo pip3 install virtualenv Create a dedicated virtual-environment where AVD will installed all required Python pakages: $ pwd /home/user/git_projects # Configure Python virtual environment $ virtualenv -p python3 .venv $ source .venv/bin/activate # Install Python requirements $ pip3 install -r ansible-avd/development/requirements.txt ...","title":"Configure your local environment"},{"location":"docs/installation/setup-environement/#setup-ansible-avd-environment","text":"Two methods can be used get Ansible up and running quickly with all the requirements to leverage ansible-avd : A Python Virtual Environment or Docker container . In both scenario, this document will leverage git approach to create a local environment with collections installed in their respective folders and additional folders for all your content. It means, all examples will be based on the following folder structure: \u251c\u2500\u2500 git_projects \u2502 \u251c\u2500\u2500 ansible-avd \u2502 \u251c\u2500\u2500 ansible-cvp \u2502 \u251c\u2500\u2500 ansible-avd-cloudvision-demo \u2502 \u251c\u2500\u2500 Makefile","title":"Setup Ansible AVD environment"},{"location":"docs/installation/setup-environement/#ansible-runner-requirements","text":"As described in requirement page , your runner should run Python 3.6.8 or Docker engine with docker-compose .","title":"Ansible runner requirements"},{"location":"docs/installation/setup-environement/#create-local-folder-structure","text":"To build local folder structure you manually run all the following commands to git clone ansible-avd , ansible-cvp collection and a repository with demo content In addition to this 3 git clone , you can also deployed a Makefile built to provide some shortcut we will discuss in a second stage. $ mkdir git_projects $ cd git_projects $ git clone https://github.com/aristanetworks/ansible-avd.git $ git clone https://github.com/aristanetworks/ansible-cvp.git $ git clone https://github.com/arista-netdevops-community/ansible-avd-cloudvision-demo.git # Copy Makefile at the root position $ cp ansible-avd/development/Makefile ./ $ make shell Or you can use a one-liner script available in ansible-avd repository to create this structure for you. This script does following actions: Create local folder for development Instantiate a local git repository (no remote) Clone AVD and CVP collections Deploy Makefile $ sh -c \" $( curl -fsSL https://get.avd.sh ) \" Because we are cloning ansible collection using git, it is recommended to read documentation about how to setup ansible to use collection based on git clone .","title":"Create local folder structure"},{"location":"docs/installation/setup-environement/#use-docker-as-avd-shell","text":"In this approach Docker container will be leveraged to provides all the AVD requirements and playbooks and collection will be shared from your localhost to the container. This approach make the run process easier as all libraries are pre-configured in container and you can continue to use your preferred text editor to edit and build your content. Considering you have deployed Makefile described in previous section, all the outputs will provide native docker command and the Make command.","title":"Use docker as AVD shell"},{"location":"docs/installation/setup-environement/#avd-environment-commands","text":"When using installation script to create your own AVD environment, a Makefile is deployed under ./ansible-arista to automate some common commands: $ make <your command>","title":"AVD environment commands"},{"location":"docs/installation/setup-environement/#commands-for-docker-compose","text":"dev-start : Start docker compose stack to develop with AVD and CVP collection (alias: start ) Deploy an mkdoc instance to expose AVD documentation with live reload for development purposes. Deploy an mkdoc instance to expose CVP documentation with live reload for development purposes. Deploy an AVD runner with a pseudo terminal connected to shell for ansible execution dev-stop : Stop docker compose stack and remove containers (alias: stop ) dev-run : Run a shell attached to ansible container (alias: shell ) dev-reload : Stop and Start docker-compose stack","title":"Commands for docker-compose"},{"location":"docs/installation/setup-environement/#commands-for-docker-only","text":"run : Run a docker container with local folder mounted under /projects . This command supports some option to test development version like: - ANSIBLE_VERSION : Specific version of ansible to install during container startup. - PIP_REQ : Specific pip requirements file to install during container startup.","title":"Commands for docker only"},{"location":"docs/installation/setup-environement/#command-for-image-management","text":"update : Get latest version of AVD runner and mkdoc servers clean : Remove avd image from local repository","title":"Command for image management"},{"location":"docs/installation/setup-environement/#run-avd-shell","text":"We are going to start a new container running ansible with all the python requirements and mount local folder under /projects . if image is missing, docker will pull out image for you automatically. $ docker run --rm -it -v $( PWD ) /:/projects avdteam/base:3.6 Unable to find image 'avdteam/base:3.6' locally 3 .6: Pulling from avdteam/base bf5952930446: Already exists 385bb58d08e6: Already exists f59c6df69726: Already exists cc14d0cfa632: Already exists f4eba3bd5be8: Already exists 55c6a5feb373: Already exists 83464a988ea4: Pull complete 9b675b85887d: Pull complete 9cce9aa068f4: Pull complete a49dbba0fea8: Pull complete 793f98fe2265: Pull complete Digest: sha256:ead3ef030caa6caeafd6ddbfd31ce935da26b66914096c9543d9a44cca993dfd Status: Downloaded newer image for avdteam/base:3.6 Agent pid 45 \u279c /projects You can use a Make command to run exact same set of actions: $ make run Unable to find image 'avdteam/base:3.6' locally 3 .6: Pulling from avdteam/base bf5952930446: Already exists 385bb58d08e6: Already exists f59c6df69726: Already exists cc14d0cfa632: Already exists f4eba3bd5be8: Already exists 55c6a5feb373: Already exists 83464a988ea4: Pull complete 9b675b85887d: Pull complete 9cce9aa068f4: Pull complete a49dbba0fea8: Pull complete 793f98fe2265: Pull complete Digest: sha256:ead3ef030caa6caeafd6ddbfd31ce935da26b66914096c9543d9a44cca993dfd Status: Downloaded newer image for avdteam/base:3.6 Agent pid 45 \u279c /projects Then you can move to your content folder as structure remains the same: \u279c /projects ls -l drwxr-xr-x 24 root root 768 Sep 4 15 :47 ansible-avd drwxr-xr-x 24 root root 768 Sep 4 15 :47 ansible-cvp drwxr-xr-x 24 root root 768 Sep 4 15 :47 ansible-avd-cloudvision-demo drwxr-xr-x 24 root root 768 Sep 4 15 :47 Makefile You can validate everything is setup correctly: \u279c /projects python --version Python 3 .6.12 \u279c /projects ansible --version ansible 2 .9.6 config file = None configured module search path = [ '/root/.ansible/plugins/modules' , '/usr/share/ansible/plugins/modules' ] ansible python module location = /root/.local/lib/python3.6/site-packages/ansible executable location = /root/.local/bin/ansible python version = 3 .6.12 ( default, Aug 18 2020 , 04 :28:43 ) [ GCC 8 .3.0 ] To exit container, just use exit \u279c /projects exit $","title":"Run AVD shell"},{"location":"docs/installation/setup-environement/#get-latest-image-of-avd-container","text":"Time to time, AVD container is updated to reflect some changes in either python requirements or ansible version. Because your docker engine won\u2019t automatically get latest version, it might be important to update manually this container: $ docker pull avdteam/base:3.6 latest: Pulling from avdteam/base 8a29a15cefae: Already exists 95df01e08bce: Downloading [============================================== > ] 33 .55MB/36.35MB 512a8a4d71f7: Downloading [========================================= > ] 45 .1MB/53.85MB 209c1657264b: Download complete bd6eece0221e: Downloading [=================== > ] 52 .04MB/132.1MB 036c486feecb: Waiting Your environment is now ready and you can start to build your own project leveraging ansible-avd and ansible-cvp collections.","title":"Get latest image of AVD container"},{"location":"docs/installation/setup-environement/#using-python-3-virtual-environment-feature","text":"This section describes how to configure python to run ansible and AVD. As a requirement, we consider python3 as default python interpreter and pip3 as package manager for python3. Some differences can be spotted depending on your own operating system and how they package python. Disclaimer : Not preferred method. if you are not an experienced user, please use docker approach. In a shell, install virtualenv package: # install virtualenv via pip3 $ sudo pip3 install virtualenv Create a dedicated virtual-environment where AVD will installed all required Python pakages: $ pwd /home/user/git_projects # Configure Python virtual environment $ virtualenv -p python3 .venv $ source .venv/bin/activate # Install Python requirements $ pip3 install -r ansible-avd/development/requirements.txt ...","title":"Using Python 3 Virtual Environment feature"},{"location":"docs/installation/setup-galaxy/","text":"Collection installation via ansible-galaxy # Install from Ansible Galaxy # arista.cvp collection is available on Ansible Galaxy server and can be automatically installed on your system. Latest version # $ ansible-galaxy collection install arista.avd Install specific version # $ ansible-galaxy collection install arista.avd: == 1 .0.2 You can specify multiple range identifiers which are split by ,. You can use the following range identifiers: * : Any version, this is the default used when no range specified is set. != : Version is not equal to the one specified. == : Version must be the one specified. >= : Version is greater than or equal to the one specified. > : Version is greater than the one specified. <= : Version is less than or equal to the one specified. < : Version is less than the one specified. Install in specific directory # If you want to install collection in a specific directory part of your project, you can call ansible-galaxy and update your ansible.cfg # Install collection under ${PWD/collections/} $ ansible-galaxy collection install arista.cvp -p collections/ # Update ansible.cfg file $ vim ansible.cfg collections_paths = ${ PWD } /collections:~/.ansible/collections:/usr/share/ansible/collections Upgrade installed AVD collection # You can use -f to force installation of a new version for any installed collection: $ ansible-galaxy collection install -f arista.cvp Process install dependency map Starting collection install process Installing 'arista.cvp:1.0.2' to '/root/.ansible/collections/ansible_collections/arista/cvp' Note: Ansible community is discussing option to implement specific triggers to support upgrade under issue #65699 Ansible resources # You can find some additional information about how to use ansible\u2019s collections on the following Ansible pages: Ansible collection user guide Ansible User guide","title":"Installation using ansible-galaxy"},{"location":"docs/installation/setup-galaxy/#collection-installation-via-ansible-galaxy","text":"","title":"Collection installation via ansible-galaxy"},{"location":"docs/installation/setup-galaxy/#install-from-ansible-galaxy","text":"arista.cvp collection is available on Ansible Galaxy server and can be automatically installed on your system.","title":"Install from Ansible Galaxy"},{"location":"docs/installation/setup-galaxy/#latest-version","text":"$ ansible-galaxy collection install arista.avd","title":"Latest version"},{"location":"docs/installation/setup-galaxy/#install-specific-version","text":"$ ansible-galaxy collection install arista.avd: == 1 .0.2 You can specify multiple range identifiers which are split by ,. You can use the following range identifiers: * : Any version, this is the default used when no range specified is set. != : Version is not equal to the one specified. == : Version must be the one specified. >= : Version is greater than or equal to the one specified. > : Version is greater than the one specified. <= : Version is less than or equal to the one specified. < : Version is less than the one specified.","title":"Install specific version"},{"location":"docs/installation/setup-galaxy/#install-in-specific-directory","text":"If you want to install collection in a specific directory part of your project, you can call ansible-galaxy and update your ansible.cfg # Install collection under ${PWD/collections/} $ ansible-galaxy collection install arista.cvp -p collections/ # Update ansible.cfg file $ vim ansible.cfg collections_paths = ${ PWD } /collections:~/.ansible/collections:/usr/share/ansible/collections","title":"Install in specific directory"},{"location":"docs/installation/setup-galaxy/#upgrade-installed-avd-collection","text":"You can use -f to force installation of a new version for any installed collection: $ ansible-galaxy collection install -f arista.cvp Process install dependency map Starting collection install process Installing 'arista.cvp:1.0.2' to '/root/.ansible/collections/ansible_collections/arista/cvp' Note: Ansible community is discussing option to implement specific triggers to support upgrade under issue #65699","title":"Upgrade installed AVD collection"},{"location":"docs/installation/setup-galaxy/#ansible-resources","text":"You can find some additional information about how to use ansible\u2019s collections on the following Ansible pages: Ansible collection user guide Ansible User guide","title":"Ansible resources"},{"location":"docs/installation/setup-git/","text":"Installation using GIT # Using GIT as source of collection in ansible provides an easy way to implement all the changes once they are part of the development branch without waiting for a new tagged version shipped to ansible-galaxy. Use Git as source of collection # In this setup, git repository will be used by ansible as collection. It is useful when working on feature development as we can change git branch and test code lively. Get repository locally # # Clone repository $ git clone https://github.com/aristanetworks/ansible-cvp.git # Move to git folder cd ansible-cvp Update your ansible.cfg # In your project, update your ansible.cfg file to point collection_paths to your local version of ansible-cvp Get full path to your newly cloned AVD repository. # Get your current location $ pwd /path/to/ansible/avd/collection_repository Configure your project to use AVD repository as source of collections: # Update your ansible.cfg in your playbook project $ vim ansible.cfg collections_paths = /path/to/ansible/cvp/collection_repository Build & install collection from git # In this approach, an ansible collection package is built from current git version and installed locally. Clone repository # $ git clone https://github.com/aristanetworks/ansible-cvp.git $ cd ansible-avd Build and install collection # This section should be used only to test collection packaging and to create an offline package to ship on your internal resources if required. $ ansible-galaxy collection build --force ansible_collections/arista/avd $ ansible-galaxy collection install arista-cvp-<VERSION>.tar.gz","title":"Installation using git"},{"location":"docs/installation/setup-git/#installation-using-git","text":"Using GIT as source of collection in ansible provides an easy way to implement all the changes once they are part of the development branch without waiting for a new tagged version shipped to ansible-galaxy.","title":"Installation using GIT"},{"location":"docs/installation/setup-git/#use-git-as-source-of-collection","text":"In this setup, git repository will be used by ansible as collection. It is useful when working on feature development as we can change git branch and test code lively.","title":"Use Git as source of collection"},{"location":"docs/installation/setup-git/#get-repository-locally","text":"# Clone repository $ git clone https://github.com/aristanetworks/ansible-cvp.git # Move to git folder cd ansible-cvp","title":"Get repository locally"},{"location":"docs/installation/setup-git/#update-your-ansiblecfg","text":"In your project, update your ansible.cfg file to point collection_paths to your local version of ansible-cvp Get full path to your newly cloned AVD repository. # Get your current location $ pwd /path/to/ansible/avd/collection_repository Configure your project to use AVD repository as source of collections: # Update your ansible.cfg in your playbook project $ vim ansible.cfg collections_paths = /path/to/ansible/cvp/collection_repository","title":"Update your ansible.cfg"},{"location":"docs/installation/setup-git/#build-install-collection-from-git","text":"In this approach, an ansible collection package is built from current git version and installed locally.","title":"Build &amp; install collection from git"},{"location":"docs/installation/setup-git/#clone-repository","text":"$ git clone https://github.com/aristanetworks/ansible-cvp.git $ cd ansible-avd","title":"Clone repository"},{"location":"docs/installation/setup-git/#build-and-install-collection","text":"This section should be used only to test collection packaging and to create an offline package to ship on your internal resources if required. $ ansible-galaxy collection build --force ansible_collections/arista/avd $ ansible-galaxy collection install arista-cvp-<VERSION>.tar.gz","title":"Build and install collection"},{"location":"docs/modules/cv_configlet.rst/","text":"cv_configlet # Create, Delete, or Update CloudVision Portal Configlets. Module added in version 2.9 Synopsis # CloudVison Portal Configlet compares the list of configlets and config in configlets against cvp-facts then adds, deletes, or updates them as appropriate. If a configlet is in cvp_facts but not in configlets it will be deleted. If a configlet is in configlets but not in cvp_facts it will be created. If a configlet is in both configlets and cvp_facts it configuration will be compared and updated with the version in configlets if the two are different. Module-specific Options # The following options may be specified for this module: parameter type required default choices comments configlet_filter list no ['none'] Filter to apply intended mode on a set of configlet. If not used, then module only uses ADD mode. configlet_filter list configlets that can be modified or deleted based on configlets entries. configlets dict yes List of configlets to managed on CVP server. configlets_notes str no Managed by Ansible Add a note to the configlets. cvp_facts dict yes Facts extracted from CVP servers using cv_facts module state str no present present absent If absent, configlets will be removed from CVP if they are not bound to either a container or a device. If present, configlets will be created or updated. Examples: # --- - name : Test cv_configlet_v2 hosts : cvp connection : local gather_facts : no vars : configlet_list : Test_Configlet : \"! This is a Very First Testing Configlet \\n !\" Test_DYNAMIC_Configlet : \"{{ lookup('file', 'templates/configlet_'+inventory_hostname+'.txt') }}\" tasks : - name : 'Collecting facts from CVP {{inventory_hostname}}.' tags : - always cv_facts : register : cvp_facts - name : 'Create configlets on CVP {{inventory_hostname}}.' tags : - provision cv_configlet : cvp_facts : \"{{cvp_facts.ansible_facts}}\" configlets : \"{{configlet_list}}\" configlets_notes : \"Configlet managed by Ansible\" configlet_filter : [ \"New\" , \"Test\" , \"base-chk\" , \"base-firewall\" ] register : cvp_configlet Author # - EMEA AS Team (@aristanetworks) Status # This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Module cv_configlet"},{"location":"docs/modules/cv_configlet.rst/#cv_configlet","text":"Create, Delete, or Update CloudVision Portal Configlets. Module added in version 2.9","title":"cv_configlet"},{"location":"docs/modules/cv_configlet.rst/#synopsis","text":"CloudVison Portal Configlet compares the list of configlets and config in configlets against cvp-facts then adds, deletes, or updates them as appropriate. If a configlet is in cvp_facts but not in configlets it will be deleted. If a configlet is in configlets but not in cvp_facts it will be created. If a configlet is in both configlets and cvp_facts it configuration will be compared and updated with the version in configlets if the two are different.","title":"Synopsis"},{"location":"docs/modules/cv_configlet.rst/#module-specific-options","text":"The following options may be specified for this module: parameter type required default choices comments configlet_filter list no ['none'] Filter to apply intended mode on a set of configlet. If not used, then module only uses ADD mode. configlet_filter list configlets that can be modified or deleted based on configlets entries. configlets dict yes List of configlets to managed on CVP server. configlets_notes str no Managed by Ansible Add a note to the configlets. cvp_facts dict yes Facts extracted from CVP servers using cv_facts module state str no present present absent If absent, configlets will be removed from CVP if they are not bound to either a container or a device. If present, configlets will be created or updated.","title":"Module-specific Options"},{"location":"docs/modules/cv_configlet.rst/#examples","text":"--- - name : Test cv_configlet_v2 hosts : cvp connection : local gather_facts : no vars : configlet_list : Test_Configlet : \"! This is a Very First Testing Configlet \\n !\" Test_DYNAMIC_Configlet : \"{{ lookup('file', 'templates/configlet_'+inventory_hostname+'.txt') }}\" tasks : - name : 'Collecting facts from CVP {{inventory_hostname}}.' tags : - always cv_facts : register : cvp_facts - name : 'Create configlets on CVP {{inventory_hostname}}.' tags : - provision cv_configlet : cvp_facts : \"{{cvp_facts.ansible_facts}}\" configlets : \"{{configlet_list}}\" configlets_notes : \"Configlet managed by Ansible\" configlet_filter : [ \"New\" , \"Test\" , \"base-chk\" , \"base-firewall\" ] register : cvp_configlet","title":"Examples:"},{"location":"docs/modules/cv_configlet.rst/#author","text":"- EMEA AS Team (@aristanetworks)","title":"Author"},{"location":"docs/modules/cv_configlet.rst/#status","text":"This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Status"},{"location":"docs/modules/cv_container.rst/","text":"cv_container # Manage Provisioning topology. Module added in version 2.9 Synopsis # CloudVision Portal Configlet configuration requires a dictionary of containers with their parent, to create and delete containers on CVP side. Returns number of created and/or deleted containers Module-specific Options # The following options may be specified for this module: parameter type required default choices comments configlet_filter list no ['none'] Filter to apply intended set of configlet on containers. If not used, then module only uses ADD mode. configlet_filter list configlets that can be modified or deleted based on configlets entries. cvp_facts dict yes Facts from CVP collected by cv_facts module mode str no merge merge override delete Allow to save topology or not topology dict yes Yaml dictionary to describe intended containers Examples: # - name : Create container topology on CVP hosts : cvp connection : local gather_facts : no vars : verbose : False containers : Fabric : parent_container : Tenant Spines : parent_container : Fabric configlets : - container_configlet images : - 4.22 . 0 F devices : - veos01 tasks : - name : \"Gather CVP facts {{inventory_hostname}}\" cv_facts : register : cvp_facts - name : \"Build Container topology on {{inventory_hostname}}\" cv_container : cvp_facts : '{{cvp_facts.ansible_facts}}' Author # - EMEA AS Team (@aristanetworks) Status # This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Module cv_container"},{"location":"docs/modules/cv_container.rst/#cv_container","text":"Manage Provisioning topology. Module added in version 2.9","title":"cv_container"},{"location":"docs/modules/cv_container.rst/#synopsis","text":"CloudVision Portal Configlet configuration requires a dictionary of containers with their parent, to create and delete containers on CVP side. Returns number of created and/or deleted containers","title":"Synopsis"},{"location":"docs/modules/cv_container.rst/#module-specific-options","text":"The following options may be specified for this module: parameter type required default choices comments configlet_filter list no ['none'] Filter to apply intended set of configlet on containers. If not used, then module only uses ADD mode. configlet_filter list configlets that can be modified or deleted based on configlets entries. cvp_facts dict yes Facts from CVP collected by cv_facts module mode str no merge merge override delete Allow to save topology or not topology dict yes Yaml dictionary to describe intended containers","title":"Module-specific Options"},{"location":"docs/modules/cv_container.rst/#examples","text":"- name : Create container topology on CVP hosts : cvp connection : local gather_facts : no vars : verbose : False containers : Fabric : parent_container : Tenant Spines : parent_container : Fabric configlets : - container_configlet images : - 4.22 . 0 F devices : - veos01 tasks : - name : \"Gather CVP facts {{inventory_hostname}}\" cv_facts : register : cvp_facts - name : \"Build Container topology on {{inventory_hostname}}\" cv_container : cvp_facts : '{{cvp_facts.ansible_facts}}'","title":"Examples:"},{"location":"docs/modules/cv_container.rst/#author","text":"- EMEA AS Team (@aristanetworks)","title":"Author"},{"location":"docs/modules/cv_container.rst/#status","text":"This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Status"},{"location":"docs/modules/cv_device.rst/","text":"cv_device # Provision, Reset, or Update CloudVision Portal Devices. Module added in version 2.9 Synopsis # CloudVison Portal Device compares the list of Devices in in devices against cvp-facts then adds, resets, or updates them as appropriate. If a device is in cvp_facts but not in devices it will be reset to factory defaults If a device is in devices but not in cvp_facts it will be provisioned If a device is in both devices and cvp_facts its configlets and imageBundles will be compared and updated with the version in devices if the two are different. Module-specific Options # The following options may be specified for this module: parameter type required default choices comments configlet_mode str no override override merge delete If override, Add listed configlets and remove all unlisted ones. If merge, Add listed configlets to device and do not touch already configured configlets. cvp_facts dict yes Facts from CVP collected by cv_facts module device_filter list no ['all'] Filter to apply intended mode on a set of configlet. If not used, then module only uses ADD mode. device_filter list devices that can be modified or deleted based on configlets entries. devices dict yes Yaml dictionary to describe intended devices configuration from CVP stand point. state str no present present absent If absent, devices will be removed from CVP and moved back to undefined. If present, devices will be configured or updated. Examples: # --- - name : Test cv_device hosts : cvp connection : local gather_facts : no collections : - arista . cvp vars : configlet_list : cv_device_test01 : \"alias a{{ 999 | random }} show version\" cv_device_test02 : \"alias a{{ 999 | random }} show version\" # Device inventory for provision activity: bind configlet devices_inventory : veos01 : name : veos01 configlets : - cv_device_test01 - SYS_TelemetryBuilderV2_172 . 23.0 . 2 _1 - veos01 - basic - configuration - SYS_TelemetryBuilderV2 tasks : # Collect CVP Facts as init process - name : \"Gather CVP facts from {{inventory_hostname}}\" cv_facts : register : cvp_facts tags : - always - name : \"Configure devices on {{inventory_hostname}}\" tags : - provision cv_device : devices : \"{{devices_inventory}}\" cvp_facts : '{{cvp_facts.ansible_facts}}' device_filter : [ 'veos' ] register : cvp_device - name : \"Add configlet to device on {{inventory_hostname}}\" tags : - provision cv_device : devices : \"{{devices_inventory}}\" cvp_facts : '{{cvp_facts.ansible_facts}}' configlet_mode : merge device_filter : [ 'veos' ] register : cvp_device Author # - EMEA AS Team (@aristanetworks) Status # This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Module cv_device"},{"location":"docs/modules/cv_device.rst/#cv_device","text":"Provision, Reset, or Update CloudVision Portal Devices. Module added in version 2.9","title":"cv_device"},{"location":"docs/modules/cv_device.rst/#synopsis","text":"CloudVison Portal Device compares the list of Devices in in devices against cvp-facts then adds, resets, or updates them as appropriate. If a device is in cvp_facts but not in devices it will be reset to factory defaults If a device is in devices but not in cvp_facts it will be provisioned If a device is in both devices and cvp_facts its configlets and imageBundles will be compared and updated with the version in devices if the two are different.","title":"Synopsis"},{"location":"docs/modules/cv_device.rst/#module-specific-options","text":"The following options may be specified for this module: parameter type required default choices comments configlet_mode str no override override merge delete If override, Add listed configlets and remove all unlisted ones. If merge, Add listed configlets to device and do not touch already configured configlets. cvp_facts dict yes Facts from CVP collected by cv_facts module device_filter list no ['all'] Filter to apply intended mode on a set of configlet. If not used, then module only uses ADD mode. device_filter list devices that can be modified or deleted based on configlets entries. devices dict yes Yaml dictionary to describe intended devices configuration from CVP stand point. state str no present present absent If absent, devices will be removed from CVP and moved back to undefined. If present, devices will be configured or updated.","title":"Module-specific Options"},{"location":"docs/modules/cv_device.rst/#examples","text":"--- - name : Test cv_device hosts : cvp connection : local gather_facts : no collections : - arista . cvp vars : configlet_list : cv_device_test01 : \"alias a{{ 999 | random }} show version\" cv_device_test02 : \"alias a{{ 999 | random }} show version\" # Device inventory for provision activity: bind configlet devices_inventory : veos01 : name : veos01 configlets : - cv_device_test01 - SYS_TelemetryBuilderV2_172 . 23.0 . 2 _1 - veos01 - basic - configuration - SYS_TelemetryBuilderV2 tasks : # Collect CVP Facts as init process - name : \"Gather CVP facts from {{inventory_hostname}}\" cv_facts : register : cvp_facts tags : - always - name : \"Configure devices on {{inventory_hostname}}\" tags : - provision cv_device : devices : \"{{devices_inventory}}\" cvp_facts : '{{cvp_facts.ansible_facts}}' device_filter : [ 'veos' ] register : cvp_device - name : \"Add configlet to device on {{inventory_hostname}}\" tags : - provision cv_device : devices : \"{{devices_inventory}}\" cvp_facts : '{{cvp_facts.ansible_facts}}' configlet_mode : merge device_filter : [ 'veos' ] register : cvp_device","title":"Examples:"},{"location":"docs/modules/cv_device.rst/#author","text":"- EMEA AS Team (@aristanetworks)","title":"Author"},{"location":"docs/modules/cv_device.rst/#status","text":"This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Status"},{"location":"docs/modules/cv_device_v1.rst/","text":"cv_device_v1 # Provision, Reset, or Update CloudVision Portal Devices. Module added in version 2.9 Synopsis # CloudVison Portal Device compares the list of Devices in in devices against cvp-facts then adds, resets, or updates them as appropriate. If a device is in cvp_facts but not in devices it will be reset to factory defaults If a device is in devices but not in cvp_facts it will be provisioned If a device is in both devices and cvp_facts its configlets and imageBundles will be compared and updated with the version in devices if the two are different. Module-specific Options # The following options may be specified for this module: parameter type required default choices comments cvp_facts dict yes Facts from CVP collected by cv_facts module device_filter list no ['none'] Filter to apply intended mode on a set of configlet. If not used, then module only uses ADD mode. device_filter list devices that can be modified or deleted based on configlets entries. devices dict yes Yaml dictionary to describe intended devices configuration from CVP stand point. state str no present present absent If absent, devices will be removed from CVP and moved back to undefined. If present, devices will be configured or updated. Examples: # --- - name : Test cv_device hosts : cvp connection : local gather_facts : no collections : - arista . cvp vars : configlet_list : cv_device_test01 : \"alias a{{ 999 | random }} show version\" cv_device_test02 : \"alias a{{ 999 | random }} show version\" # Device inventory for provision activity: bind configlet devices_inventory : veos01 : name : veos01 configlets : - cv_device_test01 - SYS_TelemetryBuilderV2_172 . 23.0 . 2 _1 - veos01 - basic - configuration - SYS_TelemetryBuilderV2 tasks : # Collect CVP Facts as init process - name : \"Gather CVP facts from {{inventory_hostname}}\" cv_facts : register : cvp_facts tags : - always - name : \"Configure devices on {{inventory_hostname}}\" tags : - provision cv_device : devices : \"{{devices_inventory}}\" cvp_facts : '{{cvp_facts.ansible_facts}}' device_filter : [ 'veos' ] register : cvp_device Author # - EMEA AS Team (@aristanetworks) Status # This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"cv\\_device\\_v1"},{"location":"docs/modules/cv_device_v1.rst/#cv_device_v1","text":"Provision, Reset, or Update CloudVision Portal Devices. Module added in version 2.9","title":"cv_device_v1"},{"location":"docs/modules/cv_device_v1.rst/#synopsis","text":"CloudVison Portal Device compares the list of Devices in in devices against cvp-facts then adds, resets, or updates them as appropriate. If a device is in cvp_facts but not in devices it will be reset to factory defaults If a device is in devices but not in cvp_facts it will be provisioned If a device is in both devices and cvp_facts its configlets and imageBundles will be compared and updated with the version in devices if the two are different.","title":"Synopsis"},{"location":"docs/modules/cv_device_v1.rst/#module-specific-options","text":"The following options may be specified for this module: parameter type required default choices comments cvp_facts dict yes Facts from CVP collected by cv_facts module device_filter list no ['none'] Filter to apply intended mode on a set of configlet. If not used, then module only uses ADD mode. device_filter list devices that can be modified or deleted based on configlets entries. devices dict yes Yaml dictionary to describe intended devices configuration from CVP stand point. state str no present present absent If absent, devices will be removed from CVP and moved back to undefined. If present, devices will be configured or updated.","title":"Module-specific Options"},{"location":"docs/modules/cv_device_v1.rst/#examples","text":"--- - name : Test cv_device hosts : cvp connection : local gather_facts : no collections : - arista . cvp vars : configlet_list : cv_device_test01 : \"alias a{{ 999 | random }} show version\" cv_device_test02 : \"alias a{{ 999 | random }} show version\" # Device inventory for provision activity: bind configlet devices_inventory : veos01 : name : veos01 configlets : - cv_device_test01 - SYS_TelemetryBuilderV2_172 . 23.0 . 2 _1 - veos01 - basic - configuration - SYS_TelemetryBuilderV2 tasks : # Collect CVP Facts as init process - name : \"Gather CVP facts from {{inventory_hostname}}\" cv_facts : register : cvp_facts tags : - always - name : \"Configure devices on {{inventory_hostname}}\" tags : - provision cv_device : devices : \"{{devices_inventory}}\" cvp_facts : '{{cvp_facts.ansible_facts}}' device_filter : [ 'veos' ] register : cvp_device","title":"Examples:"},{"location":"docs/modules/cv_device_v1.rst/#author","text":"- EMEA AS Team (@aristanetworks)","title":"Author"},{"location":"docs/modules/cv_device_v1.rst/#status","text":"This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Status"},{"location":"docs/modules/cv_facts.rst/","text":"cv_facts # Collect facts from CloudVision Portal. Module added in version 2.9 Synopsis # Returns list of devices, configlets, containers and images Module-specific Options # The following options may be specified for this module: parameter type required default choices comments facts list no ['all'] all devices containers configlets tasks List of facts to retrieve from CVP. By default, cv_facts returns facts for devices/configlets/containers/tasks Using this parameter allows user to limit scope to a subet of information. gather_subset list no ['default'] default config tasks_pending tasks_failed tasks_all When supplied, this argument will restrict the facts collected to a given subset. Possible values for this argument include all, hardware, config, and interfaces. Can specify a list of values to include a larger subset. Values can also be used with an initial ! to specify that a specific subset should not be collected. Examples: # --- tasks: - name: '#01 - Collect devices facts from {{ inventory_hostname }} ' cv_facts: facts: devices register: FACTS_DEVICES - name: '#02 - Collect devices facts (with config) from {{ inventory_hostname }} ' cv_facts: gather_subset: config facts: devices register: FACTS_DEVICES_CONFIG - name: '#03 - Collect confilgets facts from {{ inventory_hostname }} ' cv_facts: facts: configlets register: FACTS_CONFIGLETS - name: '#04 - Collect containers facts from {{ inventory_hostname }} ' cv_facts: facts: containers register: FACTS_CONTAINERS - name: '#10 - Collect ALL facts from {{ inventory_hostname }} ' cv_facts: register: FACTS Author # - EMEA AS Team (@aristanetworks) Status # This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Module cv_facts"},{"location":"docs/modules/cv_facts.rst/#cv_facts","text":"Collect facts from CloudVision Portal. Module added in version 2.9","title":"cv_facts"},{"location":"docs/modules/cv_facts.rst/#synopsis","text":"Returns list of devices, configlets, containers and images","title":"Synopsis"},{"location":"docs/modules/cv_facts.rst/#module-specific-options","text":"The following options may be specified for this module: parameter type required default choices comments facts list no ['all'] all devices containers configlets tasks List of facts to retrieve from CVP. By default, cv_facts returns facts for devices/configlets/containers/tasks Using this parameter allows user to limit scope to a subet of information. gather_subset list no ['default'] default config tasks_pending tasks_failed tasks_all When supplied, this argument will restrict the facts collected to a given subset. Possible values for this argument include all, hardware, config, and interfaces. Can specify a list of values to include a larger subset. Values can also be used with an initial ! to specify that a specific subset should not be collected.","title":"Module-specific Options"},{"location":"docs/modules/cv_facts.rst/#examples","text":"--- tasks: - name: '#01 - Collect devices facts from {{ inventory_hostname }} ' cv_facts: facts: devices register: FACTS_DEVICES - name: '#02 - Collect devices facts (with config) from {{ inventory_hostname }} ' cv_facts: gather_subset: config facts: devices register: FACTS_DEVICES_CONFIG - name: '#03 - Collect confilgets facts from {{ inventory_hostname }} ' cv_facts: facts: configlets register: FACTS_CONFIGLETS - name: '#04 - Collect containers facts from {{ inventory_hostname }} ' cv_facts: facts: containers register: FACTS_CONTAINERS - name: '#10 - Collect ALL facts from {{ inventory_hostname }} ' cv_facts: register: FACTS","title":"Examples:"},{"location":"docs/modules/cv_facts.rst/#author","text":"- EMEA AS Team (@aristanetworks)","title":"Author"},{"location":"docs/modules/cv_facts.rst/#status","text":"This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Status"},{"location":"docs/modules/cv_facts_v1.rst/","text":"cv_facts_v1 # Collect facts from CloudVision Portal. Module added in version 2.9 Synopsis # Returns the list of devices, configlets, containers and images Module-specific Options # The following options may be specified for this module: parameter type required default choices comments gather_subset list no ['default'] default config When supplied, this argument will restrict the facts collected to a given subset. Possible values for this argument include all, hardware, config, and interfaces. Can specify a list of values to include a larger subset. Values can also be used with an initial ! to specify that a specific subset should not be collected. Examples: # --- # Collect CVP Facts as init process - name: \"Gather CVP facts from {{ inventory_hostname }} \" arista.cvp.cv_facts_v1: register: cvp_facts Author # - EMEA AS Team (@aristanetworks) Status # This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"cv\\_facts\\_v1"},{"location":"docs/modules/cv_facts_v1.rst/#cv_facts_v1","text":"Collect facts from CloudVision Portal. Module added in version 2.9","title":"cv_facts_v1"},{"location":"docs/modules/cv_facts_v1.rst/#synopsis","text":"Returns the list of devices, configlets, containers and images","title":"Synopsis"},{"location":"docs/modules/cv_facts_v1.rst/#module-specific-options","text":"The following options may be specified for this module: parameter type required default choices comments gather_subset list no ['default'] default config When supplied, this argument will restrict the facts collected to a given subset. Possible values for this argument include all, hardware, config, and interfaces. Can specify a list of values to include a larger subset. Values can also be used with an initial ! to specify that a specific subset should not be collected.","title":"Module-specific Options"},{"location":"docs/modules/cv_facts_v1.rst/#examples","text":"--- # Collect CVP Facts as init process - name: \"Gather CVP facts from {{ inventory_hostname }} \" arista.cvp.cv_facts_v1: register: cvp_facts","title":"Examples:"},{"location":"docs/modules/cv_facts_v1.rst/#author","text":"- EMEA AS Team (@aristanetworks)","title":"Author"},{"location":"docs/modules/cv_facts_v1.rst/#status","text":"This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Status"},{"location":"docs/modules/cv_task.rst/","text":"cv_task # Execute or Cancel CVP Tasks. Module added in version 2.9 Synopsis # C l o u d V i s o n P o r t a l T a s k m o d u l e Module-specific Options # The following options may be specified for this module: parameter type required default choices comments state str no executed executed cancelled action to carry out on the task executed - execute tasks cancelled - cancel tasks tasks list yes CVP taskIDs to act on wait int no 0 Time to wait for tasks to transition to 'Completed' Examples: # --- - name : Execute all tasks registered in cvp_configlets variable arista . cvp . cv_task : tasks : \"{{ cvp_configlets.data.tasks }}\" - name : Cancel a list of pending tasks arista . cvp . cv_task : tasks : \"{{ cvp_configlets.data.tasks }}\" state : cancelled # Execute all pending tasks and wait for completion for 60 seconds # In order to get a list of all pending tasks, execute cv_facts first - name : Update cvp facts arista . cvp . cv_facts : - name : Execute all pending tasks and wait for completion for 60 seconds arista . cvp . cv_task : port : '{{cvp_port}}' tasks : \"{{ tasks }}\" wait : 60 Author # - EMEA AS Team (@aristanetworks) Status # This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Module cv_tasks"},{"location":"docs/modules/cv_task.rst/#cv_task","text":"Execute or Cancel CVP Tasks. Module added in version 2.9","title":"cv_task"},{"location":"docs/modules/cv_task.rst/#synopsis","text":"C l o u d V i s o n P o r t a l T a s k m o d u l e","title":"Synopsis"},{"location":"docs/modules/cv_task.rst/#module-specific-options","text":"The following options may be specified for this module: parameter type required default choices comments state str no executed executed cancelled action to carry out on the task executed - execute tasks cancelled - cancel tasks tasks list yes CVP taskIDs to act on wait int no 0 Time to wait for tasks to transition to 'Completed'","title":"Module-specific Options"},{"location":"docs/modules/cv_task.rst/#examples","text":"--- - name : Execute all tasks registered in cvp_configlets variable arista . cvp . cv_task : tasks : \"{{ cvp_configlets.data.tasks }}\" - name : Cancel a list of pending tasks arista . cvp . cv_task : tasks : \"{{ cvp_configlets.data.tasks }}\" state : cancelled # Execute all pending tasks and wait for completion for 60 seconds # In order to get a list of all pending tasks, execute cv_facts first - name : Update cvp facts arista . cvp . cv_facts : - name : Execute all pending tasks and wait for completion for 60 seconds arista . cvp . cv_task : port : '{{cvp_port}}' tasks : \"{{ tasks }}\" wait : 60","title":"Examples:"},{"location":"docs/modules/cv_task.rst/#author","text":"- EMEA AS Team (@aristanetworks)","title":"Author"},{"location":"docs/modules/cv_task.rst/#status","text":"This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Status"},{"location":"docs/release-notes/v1.x/","text":"Release Notes For Ansible CVP 1.x # Table of Contents: Release Notes For Ansible CVP 1.x Release v1.1.2 Release v1.1.1 Release v1.1.0 Release 1.0.6 Release 1.0.5 Release 1.0.4 Release 1.0.3a Release 1.0.3 Release 1.0.2 Release 1.0.1 Release 1.0.0 Release v1.1.2 # Supported CloudVision version 2018.2.5 2019.1 2020.1 Download Cloudvision appliance: Arista website Enhancements Update documentation about supported CVP versions (#195) Report better error message when device not reachable (#205) Add role to support configlets synchronisation between cloudvision servers (#196) Allow dhcp_configuration role to only generate dhcpd.conf file (#206) Publish collection documentation on github.io Fixed issues Fix an issue where cv_device was not able to move device and update device in same execution. (#199) Fix an issue where cv_configlet did not release list of configlets (#211) Fix an issue where CV returns empty tasks for cv_device update with no change (#217) Fix an issue where cv_configlet did not update configlet when filter was not set (#215) Contributors @noredistribution @jrecchia1029 @Hugh-Adams @guillaumeVilar Release v1.1.1 # Supported CloudVision version 2018.2.5 2019.1 2020.1 Download Cloudvision appliance: Arista website Fixed issues Fix failure when cv_container tried to attached first configlet to a container: #190 Enhancements Ansible linting improvement Update documentation to reflect supported version of Cloudvision. For detailed information please see the release tag Release v1.1.0 # Supported CloudVision version 2018.2.5 2019.x 2020.1 (#129) Download Cloudvision appliance: Arista website Enhancement Optimize cv_facts execution: #147 cv_configlet now support deletion mode to remove a list of configlets: #168 cv_device now supports mode to apply configlets (merge/override/delete): #126 cv_configlet returns diff computed by Cloudvision: #121 cv_configlet now supports custom comment to configure on CV side: #186 Support DHCP package installation for centos and ubuntu platform #172 (cherry-picked from releases/v1.0.x ) Fixed issues Fix an issue related to container topology deletion in a relative path #175 Fix an issue related to authentication fallback mechanism #183 / #185 Fix an error collecting facts if a configlet or a configlet builder contains errorCode string: #178 Others Support generic logging in module for better bug analysis: #124 Ansible lint over DCHP configuration role. Contributors @noredistribution @Hugh-Adams @b-abadie @ksator For detailed information please see the release tag Release 1.0.6 # Supported CloudVision version 2018.2.5 2019.1.x Download Cloudvision appliance: Arista website Enhancement Update DHCP role to support DHCP configuration outside of CVP: #151 Fixed issues When a container has a new configlet, it is applied to other containers: #165 / #170 For detailed information please see the release tag Release 1.0.5 # Supported CloudVision versions: 2018.2.5 2019.1.x Download Cloudvision appliance: Arista website Enhancements New Ansible role to configure ZTP service on Cloudvision: arista.cvp.ztp_configuration Implement new logging mechanism across all arista.cvp modules (Issue: #124 / PR: #146) Enable Continuous Integration using Github Actions (#148) Fixed issues: Fix an issue where cv_configlet do not detect small changes in configlet. (#135) Fix a an issue where containers were not correctly match in cv_container (#142) Fix an issue where package dependencies were broken in Dockerfile Update documentation to remove deprecated options Others Remove Dockefile for python2.7. Update Makefile to only support Dockerfile with Python3. Remove old testing files. For detailed information please see the release tag Release 1.0.4 # Supported CloudVision version: 2018.2.5 2019.1.0 2019.1.1 2019.1.2 Enhancement: Add Configlet diff in cv_configlet: Issue #120 For detailed information please see the release tag Release 1.0.3a # Supported CloudVision version: 2018.2.5 2019.1.0 2019.1.1 2019.1.2 Enhancement: Support Python 3.7 and 3.8 (Issue #105) Ship a development container (Issue #107) Optimize docker image creation Support custom name for the root container in cv_container (#113) Fixed issues: Fix an issue where cv_container did not return correct list of attached configlets (Issue #108) Upgrade SSL lib in docker images (#112) Remove save_topology from cv_container (#116) Fix an issue where cv_facts does not create correct tasks list (#118) Fix Galaxy Version from 1.0.3 For detailed information please see the release tag Release 1.0.3 # Supported CloudVision version: 2018.2.5 2019.1.0 2019.1.1 2019.1.2 Enhancement: Support Python 3.7 and 3.8 (Issue #105) Ship a development container (Issue #107) Optimize docker image creation Support custom name for the root container in cv_container (#113) Fixed issues: Fix an issue where cv_container did not return correct list of attached configlets (Issue #108) Upgrade SSL lib in docker images (#112) Remove save_topology from cv_container (#116) Fix an issue where cv_facts does not create correct tasks list (#118) For detailed information please see the release tag Release 1.0.2 # Supported CloudVision version: 2018.2.5 2019.1.0 2019.1.1 Enhancement: Use CloudVision JSON in facts (#30) Refactor cv_facts (#95) Implement subset and facts in cv_facts Implement mode in cv_container Implement state=absent in cv_configlet and cv_device Integration with Arista Validated Design project Fixed issues: Tasks not captures by cv_task from cv_device (#79) Remove duplicate taskIDs (#82) Fix some code quality Update documentation For detailed information please see the release tag Release 1.0.1 # Ansible Collection update - Improve documentation (#70) - Validate code with ansible-test for galaxy integration (#64) - Fix an issue where inactive devices could break cv_facts (#61) - Implement mode mechanism in cv_container (#58) - Fix issue where tasks appeared multiple times causing cv_task error (#82) - cv_task not catching tasks returned by cv_device (#79) Repository Update - Create Dockerfile for both python 2.7 and 3.x (#56) - Create Makefile to reduce manual overhead (#34) - Update collection path to allow local development (#73) - Create dev unit testing with Makefile For detailed information please see the release tag Release 1.0.0 # First public version of the collection. New Features: Support for CVP 2018.2.x and CVP 2019.1.x Implement cv_facts module for information gathering Implement cv_container and cv_device to manage provisioning topology Implement cv_configlet to manage configlet content on CloudVision platform Use Ansible collection approach Documentation Modules documentation: docs/ folder Example playbooks available in examples/ folder For detailed information please see the release tag","title":"Versions 1.x"},{"location":"docs/release-notes/v1.x/#release-notes-for-ansible-cvp-1x","text":"Table of Contents: Release Notes For Ansible CVP 1.x Release v1.1.2 Release v1.1.1 Release v1.1.0 Release 1.0.6 Release 1.0.5 Release 1.0.4 Release 1.0.3a Release 1.0.3 Release 1.0.2 Release 1.0.1 Release 1.0.0","title":"Release Notes For Ansible CVP 1.x"},{"location":"docs/release-notes/v1.x/#release-v112","text":"Supported CloudVision version 2018.2.5 2019.1 2020.1 Download Cloudvision appliance: Arista website Enhancements Update documentation about supported CVP versions (#195) Report better error message when device not reachable (#205) Add role to support configlets synchronisation between cloudvision servers (#196) Allow dhcp_configuration role to only generate dhcpd.conf file (#206) Publish collection documentation on github.io Fixed issues Fix an issue where cv_device was not able to move device and update device in same execution. (#199) Fix an issue where cv_configlet did not release list of configlets (#211) Fix an issue where CV returns empty tasks for cv_device update with no change (#217) Fix an issue where cv_configlet did not update configlet when filter was not set (#215) Contributors @noredistribution @jrecchia1029 @Hugh-Adams @guillaumeVilar","title":"Release v1.1.2"},{"location":"docs/release-notes/v1.x/#release-v111","text":"Supported CloudVision version 2018.2.5 2019.1 2020.1 Download Cloudvision appliance: Arista website Fixed issues Fix failure when cv_container tried to attached first configlet to a container: #190 Enhancements Ansible linting improvement Update documentation to reflect supported version of Cloudvision. For detailed information please see the release tag","title":"Release v1.1.1"},{"location":"docs/release-notes/v1.x/#release-v110","text":"Supported CloudVision version 2018.2.5 2019.x 2020.1 (#129) Download Cloudvision appliance: Arista website Enhancement Optimize cv_facts execution: #147 cv_configlet now support deletion mode to remove a list of configlets: #168 cv_device now supports mode to apply configlets (merge/override/delete): #126 cv_configlet returns diff computed by Cloudvision: #121 cv_configlet now supports custom comment to configure on CV side: #186 Support DHCP package installation for centos and ubuntu platform #172 (cherry-picked from releases/v1.0.x ) Fixed issues Fix an issue related to container topology deletion in a relative path #175 Fix an issue related to authentication fallback mechanism #183 / #185 Fix an error collecting facts if a configlet or a configlet builder contains errorCode string: #178 Others Support generic logging in module for better bug analysis: #124 Ansible lint over DCHP configuration role. Contributors @noredistribution @Hugh-Adams @b-abadie @ksator For detailed information please see the release tag","title":"Release v1.1.0"},{"location":"docs/release-notes/v1.x/#release-106","text":"Supported CloudVision version 2018.2.5 2019.1.x Download Cloudvision appliance: Arista website Enhancement Update DHCP role to support DHCP configuration outside of CVP: #151 Fixed issues When a container has a new configlet, it is applied to other containers: #165 / #170 For detailed information please see the release tag","title":"Release 1.0.6"},{"location":"docs/release-notes/v1.x/#release-105","text":"Supported CloudVision versions: 2018.2.5 2019.1.x Download Cloudvision appliance: Arista website Enhancements New Ansible role to configure ZTP service on Cloudvision: arista.cvp.ztp_configuration Implement new logging mechanism across all arista.cvp modules (Issue: #124 / PR: #146) Enable Continuous Integration using Github Actions (#148) Fixed issues: Fix an issue where cv_configlet do not detect small changes in configlet. (#135) Fix a an issue where containers were not correctly match in cv_container (#142) Fix an issue where package dependencies were broken in Dockerfile Update documentation to remove deprecated options Others Remove Dockefile for python2.7. Update Makefile to only support Dockerfile with Python3. Remove old testing files. For detailed information please see the release tag","title":"Release 1.0.5"},{"location":"docs/release-notes/v1.x/#release-104","text":"Supported CloudVision version: 2018.2.5 2019.1.0 2019.1.1 2019.1.2 Enhancement: Add Configlet diff in cv_configlet: Issue #120 For detailed information please see the release tag","title":"Release 1.0.4"},{"location":"docs/release-notes/v1.x/#release-103a","text":"Supported CloudVision version: 2018.2.5 2019.1.0 2019.1.1 2019.1.2 Enhancement: Support Python 3.7 and 3.8 (Issue #105) Ship a development container (Issue #107) Optimize docker image creation Support custom name for the root container in cv_container (#113) Fixed issues: Fix an issue where cv_container did not return correct list of attached configlets (Issue #108) Upgrade SSL lib in docker images (#112) Remove save_topology from cv_container (#116) Fix an issue where cv_facts does not create correct tasks list (#118) Fix Galaxy Version from 1.0.3 For detailed information please see the release tag","title":"Release 1.0.3a"},{"location":"docs/release-notes/v1.x/#release-103","text":"Supported CloudVision version: 2018.2.5 2019.1.0 2019.1.1 2019.1.2 Enhancement: Support Python 3.7 and 3.8 (Issue #105) Ship a development container (Issue #107) Optimize docker image creation Support custom name for the root container in cv_container (#113) Fixed issues: Fix an issue where cv_container did not return correct list of attached configlets (Issue #108) Upgrade SSL lib in docker images (#112) Remove save_topology from cv_container (#116) Fix an issue where cv_facts does not create correct tasks list (#118) For detailed information please see the release tag","title":"Release 1.0.3"},{"location":"docs/release-notes/v1.x/#release-102","text":"Supported CloudVision version: 2018.2.5 2019.1.0 2019.1.1 Enhancement: Use CloudVision JSON in facts (#30) Refactor cv_facts (#95) Implement subset and facts in cv_facts Implement mode in cv_container Implement state=absent in cv_configlet and cv_device Integration with Arista Validated Design project Fixed issues: Tasks not captures by cv_task from cv_device (#79) Remove duplicate taskIDs (#82) Fix some code quality Update documentation For detailed information please see the release tag","title":"Release 1.0.2"},{"location":"docs/release-notes/v1.x/#release-101","text":"Ansible Collection update - Improve documentation (#70) - Validate code with ansible-test for galaxy integration (#64) - Fix an issue where inactive devices could break cv_facts (#61) - Implement mode mechanism in cv_container (#58) - Fix issue where tasks appeared multiple times causing cv_task error (#82) - cv_task not catching tasks returned by cv_device (#79) Repository Update - Create Dockerfile for both python 2.7 and 3.x (#56) - Create Makefile to reduce manual overhead (#34) - Update collection path to allow local development (#73) - Create dev unit testing with Makefile For detailed information please see the release tag","title":"Release 1.0.1"},{"location":"docs/release-notes/v1.x/#release-100","text":"First public version of the collection. New Features: Support for CVP 2018.2.x and CVP 2019.1.x Implement cv_facts module for information gathering Implement cv_container and cv_device to manage provisioning topology Implement cv_configlet to manage configlet content on CloudVision platform Use Ansible collection approach Documentation Modules documentation: docs/ folder Example playbooks available in examples/ folder For detailed information please see the release tag","title":"Release 1.0.0"},{"location":"docs/release-notes/v2.x/","text":"Release Notes For Ansible CVP 2.x # Table of Contents: Release Notes For Ansible CVP 2.x Release 2.1.0 Release 2.0.0 Release 2.1.0 # Supported CloudVision version: 2018.2.5 2019.1 2020.1 2020.2 Cloudvision as a Service Download Cloudvision appliance: Arista website Fixed issues N/A Enhancements PR: Add CVaaS support (#235) Documentation updates PR: Add HOW-TO section (#237) For detailed information please see the release tag Release 2.0.0 # Supported CloudVision version: 2018.2.5 2019.1 2020.1 2020.2 Download Cloudvision appliance: Arista website Enhancements Move collection backend to cvprac module to manage all Cloudvision communications. - Please update python requirements by installing cvprac in version 1.0.4 - AVD container has been updated accordingly. Please pull out the new version on your laptop. Install cvprac pip install cvprac == 1 .0.4 Update Docker image docker pull avdteam/base:3.6 Contributors @noredistribution @Hugh-Adams @carlbuchmann @mharista For detailed information please see the release tag","title":"Versions 2.x"},{"location":"docs/release-notes/v2.x/#release-notes-for-ansible-cvp-2x","text":"Table of Contents: Release Notes For Ansible CVP 2.x Release 2.1.0 Release 2.0.0","title":"Release Notes For Ansible CVP 2.x"},{"location":"docs/release-notes/v2.x/#release-210","text":"Supported CloudVision version: 2018.2.5 2019.1 2020.1 2020.2 Cloudvision as a Service Download Cloudvision appliance: Arista website Fixed issues N/A Enhancements PR: Add CVaaS support (#235) Documentation updates PR: Add HOW-TO section (#237) For detailed information please see the release tag","title":"Release 2.1.0"},{"location":"docs/release-notes/v2.x/#release-200","text":"Supported CloudVision version: 2018.2.5 2019.1 2020.1 2020.2 Download Cloudvision appliance: Arista website Enhancements Move collection backend to cvprac module to manage all Cloudvision communications. - Please update python requirements by installing cvprac in version 1.0.4 - AVD container has been updated accordingly. Please pull out the new version on your laptop. Install cvprac pip install cvprac == 1 .0.4 Update Docker image docker pull avdteam/base:3.6 Contributors @noredistribution @Hugh-Adams @carlbuchmann @mharista For detailed information please see the release tag","title":"Release 2.0.0"},{"location":"molecule/","text":"CVP Unit test # This section provides a list of Ansible Cloudvision scenario executed during Continuous Integration to validate CVP integration. Ansible molecule # Molecule provides support for testing with multiple instances, operating systems and distributions, virtualization providers, test frameworks and testing scenarios. Molecule encourages an approach that results in consistently developed roles that are well-written, easily understood and maintained. Scenario # Current molecule implementation provides following scenario: dhcp_configuration Manual execution # To manually run molecule testing, follow commands: # Install development requirements $ pip install -r development/requirements-dev.txt # Move to AVD collection $ ansible-avd/ansible_collections/arista/cvp # Run molecule for a given test $ molecule test -s <scenario-name> # Run molecule for all test $ molecule test --all Continuous Integration # These scenario are all included in github actions and executed on push and pull_request when a file under roles and/or molecule is updated. name : Ansible Molecule on : push : pull_request : paths : - 'ansible_collections/arista/cvp/roles/**' - 'ansible_collections/arista/cvp/molecules/**' - 'requirements.txt' jobs : molecule : runs-on : ubuntu-latest env : PY_COLORS : 1 # allows molecule colors to be passed to GitHub Actions ANSIBLE_FORCE_COLOR : 1 # allows ansible colors to be passed to GitHub Actions strategy : fail-fast : true matrix : avd_scenario : - dhcp_configuration steps : - name : Checkout repository uses : actions/checkout@v2 - name : Run molecule action uses : inetsix/molecule-collection-actions@master with : molecule_parentdir : 'ansible_collections/arista/cvp' molecule_command : 'test' molecule_args : '-s ${{ matrix.avd_scenario }}' pip_file : 'requirements.txt'","title":"CVP Unit test"},{"location":"molecule/#cvp-unit-test","text":"This section provides a list of Ansible Cloudvision scenario executed during Continuous Integration to validate CVP integration.","title":"CVP Unit test"},{"location":"molecule/#ansible-molecule","text":"Molecule provides support for testing with multiple instances, operating systems and distributions, virtualization providers, test frameworks and testing scenarios. Molecule encourages an approach that results in consistently developed roles that are well-written, easily understood and maintained.","title":"Ansible molecule"},{"location":"molecule/#scenario","text":"Current molecule implementation provides following scenario: dhcp_configuration","title":"Scenario"},{"location":"molecule/#manual-execution","text":"To manually run molecule testing, follow commands: # Install development requirements $ pip install -r development/requirements-dev.txt # Move to AVD collection $ ansible-avd/ansible_collections/arista/cvp # Run molecule for a given test $ molecule test -s <scenario-name> # Run molecule for all test $ molecule test --all","title":"Manual execution"},{"location":"molecule/#continuous-integration","text":"These scenario are all included in github actions and executed on push and pull_request when a file under roles and/or molecule is updated. name : Ansible Molecule on : push : pull_request : paths : - 'ansible_collections/arista/cvp/roles/**' - 'ansible_collections/arista/cvp/molecules/**' - 'requirements.txt' jobs : molecule : runs-on : ubuntu-latest env : PY_COLORS : 1 # allows molecule colors to be passed to GitHub Actions ANSIBLE_FORCE_COLOR : 1 # allows ansible colors to be passed to GitHub Actions strategy : fail-fast : true matrix : avd_scenario : - dhcp_configuration steps : - name : Checkout repository uses : actions/checkout@v2 - name : Run molecule action uses : inetsix/molecule-collection-actions@master with : molecule_parentdir : 'ansible_collections/arista/cvp' molecule_command : 'test' molecule_args : '-s ${{ matrix.avd_scenario }}' pip_file : 'requirements.txt'","title":"Continuous Integration"},{"location":"plugins/","text":"Collections Plugins Directory # arista.cvp collection provides a set of plugins to configure Arista EOS devices with a CloudVision Platform server. List of available modules # arista.cvp.cv_facts - Collect CVP facts from server like list of containers, devices, configlet and tasks. arista.cvp.cv_configlet : Manage configlet configured on CVP. arista.cvp.cv_container : Manage container topology and attach configlet and devices to containers. arista.cvp.cv_device : Manage devices configured on CVP arista.cvp.cv_task : Run tasks created on CVP.","title":"Collections Plugins Directory"},{"location":"plugins/#collections-plugins-directory","text":"arista.cvp collection provides a set of plugins to configure Arista EOS devices with a CloudVision Platform server.","title":"Collections Plugins Directory"},{"location":"plugins/#list-of-available-modules","text":"arista.cvp.cv_facts - Collect CVP facts from server like list of containers, devices, configlet and tasks. arista.cvp.cv_configlet : Manage configlet configured on CVP. arista.cvp.cv_container : Manage container topology and attach configlet and devices to containers. arista.cvp.cv_device : Manage devices configured on CVP arista.cvp.cv_task : Run tasks created on CVP.","title":"List of available modules"},{"location":"roles/configlets_sync/","text":"configlet_sync role # Ansible role to synchronize configlets between 2 instances of Cloudvision servers. This role synchronize a designated set of CVP Configlets across multiple CVP instances. This ability to synchronize Configlets provides an efficient way of ensuring organizational policies and security requirements can be quickly deployed across an entire Arista estate in a consistent automated manner. The aim will be to deploy and synchronize a set of configlets that could be updated from any instance of CVP or via an Ansible PlayBook. This provides the most flexible method of managing the configlets without imposing any requirements to exclusively use either CVP or Ansible for updating them. This role is implementation of example describe on EOS Central blog by @Hugh-Adams. Requirements # No specific requirements to use this role. Tested Platforms # Any version of Cloudvision supported by current arista.cvp collection. Role Variables # Mandatory variables # --- action : < * action to run with the role : init|sync|pull|push > init : Create initial local folder to save role outputs. pull : Connects to each of the CVP instances, locates the configlets with {{configlet_filter}} in their name and updates local shared configlet data (config, last time changed, associated containers, associated devices) for each CVP instance. pull : Connects to each of the CVP instances and updates the shared configlets on each one using the information provided by pull action. sync : Execute both pull and push actions. Optional variables # Role variables # configlet_filter : < PREFIX of configlet to look for synchronization, Default is shared > Local folders outputs # --- cvpsync_data : < Local folder where configlets_sync save data. Default is generated_vars/ > common_configlets_dir : < Folder for common configlets. Default is {{cvpsync_data}}/common_configlets/ > cvp_servers_dir : < Folder for common configlets. Default is {{cvpsync_data}}/common_configlets/ > common_configlets_dir : contains details of the configlets that are to be synchronized across the CVP instances. cvp_servers_dir : contains details of which configlets are to be deployed to the CVP servers. This directory can be used by other PlayBooks to deploy configlets and provision devices. Dependencies # No dependency required for this role. Example Playbook # Playbook # Below is a basic playbook running arista.cvp.dhcp_configuration role to only get shared configlets with generic string in the name. --- - name : Shared Configlets across CVP clusters hosts : cvp_sync serial : true gather_facts : no collections : - arista.cvp tasks : - name : 'Init Configlets Sync structure' import_role : name : configlets_sync vars : action : init - name : 'Sync Shared Configlets' import_role : name : configlets_sync vars : action : pull configlet_filter : 'generic' Example playbook to to synchronize shared configlets with generic string in the name: --- - name : Shared Configlets across CVP clusters hosts : cvp_sync serial : true gather_facts : no collections : - arista.cvp tasks : - name : 'Init Configlets Sync structure' import_role : name : configlets_sync vars : configlet_filter : 'generic' action : sync Inventory # {{inventory_name}} for Cloudvision instances can be changed to match your own environement. Only group name cvp_sync must be the same in inventory and playbook. --- all : children : cvp_sync : hosts : cv_server1 : ansible_host : 1.1.1.1.1 ansible_user : ansible ansible_password : ansible cv_server2 : ansible_host : 8.8.8.8 ansible_user : arista ansible_password : arista vars : ansible_httpapi_host : '{{ ansible_host }}' ansible_connection : httpapi ansible_httpapi_use_ssl : true ansible_httpapi_validate_certs : false ansible_network_os : eos ansible_httpapi_port : 443 # Optional - Configuration to get Virtual Env information ansible_python_interpreter : $(which python) License # Project is published under Apache 2.0 License","title":"Cloudvision Configlet Sync"},{"location":"roles/configlets_sync/#configlet_sync-role","text":"Ansible role to synchronize configlets between 2 instances of Cloudvision servers. This role synchronize a designated set of CVP Configlets across multiple CVP instances. This ability to synchronize Configlets provides an efficient way of ensuring organizational policies and security requirements can be quickly deployed across an entire Arista estate in a consistent automated manner. The aim will be to deploy and synchronize a set of configlets that could be updated from any instance of CVP or via an Ansible PlayBook. This provides the most flexible method of managing the configlets without imposing any requirements to exclusively use either CVP or Ansible for updating them. This role is implementation of example describe on EOS Central blog by @Hugh-Adams.","title":"configlet_sync role"},{"location":"roles/configlets_sync/#requirements","text":"No specific requirements to use this role.","title":"Requirements"},{"location":"roles/configlets_sync/#tested-platforms","text":"Any version of Cloudvision supported by current arista.cvp collection.","title":"Tested Platforms"},{"location":"roles/configlets_sync/#role-variables","text":"","title":"Role Variables"},{"location":"roles/configlets_sync/#mandatory-variables","text":"--- action : < * action to run with the role : init|sync|pull|push > init : Create initial local folder to save role outputs. pull : Connects to each of the CVP instances, locates the configlets with {{configlet_filter}} in their name and updates local shared configlet data (config, last time changed, associated containers, associated devices) for each CVP instance. pull : Connects to each of the CVP instances and updates the shared configlets on each one using the information provided by pull action. sync : Execute both pull and push actions.","title":"Mandatory variables"},{"location":"roles/configlets_sync/#optional-variables","text":"","title":"Optional variables"},{"location":"roles/configlets_sync/#role-variables_1","text":"configlet_filter : < PREFIX of configlet to look for synchronization, Default is shared >","title":"Role variables"},{"location":"roles/configlets_sync/#local-folders-outputs","text":"--- cvpsync_data : < Local folder where configlets_sync save data. Default is generated_vars/ > common_configlets_dir : < Folder for common configlets. Default is {{cvpsync_data}}/common_configlets/ > cvp_servers_dir : < Folder for common configlets. Default is {{cvpsync_data}}/common_configlets/ > common_configlets_dir : contains details of the configlets that are to be synchronized across the CVP instances. cvp_servers_dir : contains details of which configlets are to be deployed to the CVP servers. This directory can be used by other PlayBooks to deploy configlets and provision devices.","title":"Local folders outputs"},{"location":"roles/configlets_sync/#dependencies","text":"No dependency required for this role.","title":"Dependencies"},{"location":"roles/configlets_sync/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/configlets_sync/#playbook","text":"Below is a basic playbook running arista.cvp.dhcp_configuration role to only get shared configlets with generic string in the name. --- - name : Shared Configlets across CVP clusters hosts : cvp_sync serial : true gather_facts : no collections : - arista.cvp tasks : - name : 'Init Configlets Sync structure' import_role : name : configlets_sync vars : action : init - name : 'Sync Shared Configlets' import_role : name : configlets_sync vars : action : pull configlet_filter : 'generic' Example playbook to to synchronize shared configlets with generic string in the name: --- - name : Shared Configlets across CVP clusters hosts : cvp_sync serial : true gather_facts : no collections : - arista.cvp tasks : - name : 'Init Configlets Sync structure' import_role : name : configlets_sync vars : configlet_filter : 'generic' action : sync","title":"Playbook"},{"location":"roles/configlets_sync/#inventory","text":"{{inventory_name}} for Cloudvision instances can be changed to match your own environement. Only group name cvp_sync must be the same in inventory and playbook. --- all : children : cvp_sync : hosts : cv_server1 : ansible_host : 1.1.1.1.1 ansible_user : ansible ansible_password : ansible cv_server2 : ansible_host : 8.8.8.8 ansible_user : arista ansible_password : arista vars : ansible_httpapi_host : '{{ ansible_host }}' ansible_connection : httpapi ansible_httpapi_use_ssl : true ansible_httpapi_validate_certs : false ansible_network_os : eos ansible_httpapi_port : 443 # Optional - Configuration to get Virtual Env information ansible_python_interpreter : $(which python)","title":"Inventory"},{"location":"roles/configlets_sync/#license","text":"Project is published under Apache 2.0 License","title":"License"},{"location":"roles/dhcp_configuration/","text":"dhcp_configuration role # Ansible role to provision and configure Zero Touch Provisioning on a CloudVision server. Role will do the following: Install DHCP package Activate DHCPd service on CloudVision. Create /etc/dhcp/dhcpd.conf file with relevant information. Reload dhcpd service to apply changes. Requirements # No specific requirements to use this role. Tested Platforms # Below is a list of platforms where DHCPd configuration has been tested: Centos 7 / 8 Ubuntu 18.02 Arista Cloudvision 2019 and onward (for lab purpose) This role should work on any platform running ISC-DHCP server . If role is applied to Cloudvision server, DHCP configuration may be erased during upgrade process. Use it at your own risk in a production environment. Role Variables # mode : < offline/online - Select if role configure a DHCP server or just generate dhcpd.conf file locally. (default online) > # Offline only variables output_dir : < path where to save dhcpd.conf file when using offline mode.> # Online only variables dhcp_packages : [] < List of packages to install as part of DHCP service. (default is ['dhcp'])> dhcp_packages_state : < Flag to install or remove DHCP package. (default is present)> dhcp_config_dir : < Folder where dhcp config is saved. (default is /etc/dhcp/)> dhcp_config : < Configuration file for DHCP service. (default is {{ dhcp_config_dir }}/dhcpd.conf)> dhcp_service : < Name of the service running on the system for DHCP. (default is dhcpd)> # Data for template engine. For both offline and online mode ztp : default : < Section with default value for hosts configuration > registration : < * Default URL to get Script to register to CV or initial configuration > gateway : < Gateway to use by default if not set per device > nameservers : < List of default NS to use on a per host basis > general : < Section to define subnets parameters > subnets : - network : < * Subnet where DHCP will listen for request > netmask : < * Netmask of given subnet > gateway : < Gateway to configure for given subnet > nameservers : < List of name-servers to configure for given subnet > start : < First IP available in the pool > end : < Last IP available in the pool > lease_time : < Maximum lease time before device loose IP. Renewal is max/2 > clients : < List of clients on a mac-address basis > - name : < * Hostname to provide when device do a DHCP request > mac : < * Mac address of the host. Mac address value MUST be protected by either single or dual quotes > ip4 : < * IP Address of the host > registration : < Registration URL to use for the host. If not set, default value will be applied > gateway : < Gateway to use for the host. If not set, default value will be applied > nameservers : < List of NS to use for the host. If not set, default value will be applied > Variables with * are mandatory, others are optional and might be skipped if not needed in your setup. Dependencies # No dependency required for this role. Example Playbook # Generate DHCPD configuration to deploy on a DHCP server # --- - name : Configure DHCP service on CloudVision hosts : dhcp_server gather_facts : false collection : - arista.cvp vars : ztp : default : registration : 'http://10.255.0.1/ztp/bootstrap' gateway : 10.255.0.3 nameservers : - '10.255.0.3' general : subnets : - network : 10.255.0.0 netmask : 255.255.255.0 gateway : 10.255.0.3 nameservers : - '10.255.0.3' start : 10.255.0.200 end : 10.255.0.250 lease_time : 300 clients : - name : DC1-SPINE1 mac : '0c:1d:c0:1d:62:01' ip4 : 10.255.0.11 - name : DC1-SPINE2 mac : '0c:1d:c0:1d:62:02' ip4 : 10.255.0.12 - name : DC1-LEAF1A mac : '0c:1d:c0:1d:62:11' ip4 : 10.255.0.13 tasks : - name : 'Execute DHCP configuration role' import_role : name : arista.cvp.dhcp_configuration vars : mode : offline output_dirs : '{{inventory}}' Configure Server to act as ZTP server # Below is a basic playbook running arista.cvp.dhcp_configuration role --- - name : Configure DHCP service on CloudVision hosts : dhcp_server gather_facts : true collection : - arista.cvp vars : ztp : default : registration : 'http://10.255.0.1/ztp/bootstrap' gateway : 10.255.0.3 nameservers : - '10.255.0.3' general : subnets : - network : 10.255.0.0 netmask : 255.255.255.0 gateway : 10.255.0.3 nameservers : - '10.255.0.3' start : 10.255.0.200 end : 10.255.0.250 lease_time : 300 clients : - name : DC1-SPINE1 mac : '0c:1d:c0:1d:62:01' ip4 : 10.255.0.11 - name : DC1-SPINE2 mac : '0c:1d:c0:1d:62:02' ip4 : 10.255.0.12 - name : DC1-LEAF1A mac : '0c:1d:c0:1d:62:11' ip4 : 10.255.0.13 tasks : - name : 'Execute DHCP configuration role' import_role : name : arista.cvp.dhcp_configuration Inventory is configured like below: --- all : children : CVP : hosts : dhcp_server : ansible_host : 1.1.1.1 ansible_user : user1 ansible_password : password1 ansible_become_password : password1 ansible_python_interpreter : $(which python3) If you are not using root user, configure ansible_become_password since role always use become: true . SSH connection is managed by paramiko . License # Project is published under Apache 2.0 License","title":"DHCP Configuration role"},{"location":"roles/dhcp_configuration/#dhcp_configuration-role","text":"Ansible role to provision and configure Zero Touch Provisioning on a CloudVision server. Role will do the following: Install DHCP package Activate DHCPd service on CloudVision. Create /etc/dhcp/dhcpd.conf file with relevant information. Reload dhcpd service to apply changes.","title":"dhcp_configuration role"},{"location":"roles/dhcp_configuration/#requirements","text":"No specific requirements to use this role.","title":"Requirements"},{"location":"roles/dhcp_configuration/#tested-platforms","text":"Below is a list of platforms where DHCPd configuration has been tested: Centos 7 / 8 Ubuntu 18.02 Arista Cloudvision 2019 and onward (for lab purpose) This role should work on any platform running ISC-DHCP server . If role is applied to Cloudvision server, DHCP configuration may be erased during upgrade process. Use it at your own risk in a production environment.","title":"Tested Platforms"},{"location":"roles/dhcp_configuration/#role-variables","text":"mode : < offline/online - Select if role configure a DHCP server or just generate dhcpd.conf file locally. (default online) > # Offline only variables output_dir : < path where to save dhcpd.conf file when using offline mode.> # Online only variables dhcp_packages : [] < List of packages to install as part of DHCP service. (default is ['dhcp'])> dhcp_packages_state : < Flag to install or remove DHCP package. (default is present)> dhcp_config_dir : < Folder where dhcp config is saved. (default is /etc/dhcp/)> dhcp_config : < Configuration file for DHCP service. (default is {{ dhcp_config_dir }}/dhcpd.conf)> dhcp_service : < Name of the service running on the system for DHCP. (default is dhcpd)> # Data for template engine. For both offline and online mode ztp : default : < Section with default value for hosts configuration > registration : < * Default URL to get Script to register to CV or initial configuration > gateway : < Gateway to use by default if not set per device > nameservers : < List of default NS to use on a per host basis > general : < Section to define subnets parameters > subnets : - network : < * Subnet where DHCP will listen for request > netmask : < * Netmask of given subnet > gateway : < Gateway to configure for given subnet > nameservers : < List of name-servers to configure for given subnet > start : < First IP available in the pool > end : < Last IP available in the pool > lease_time : < Maximum lease time before device loose IP. Renewal is max/2 > clients : < List of clients on a mac-address basis > - name : < * Hostname to provide when device do a DHCP request > mac : < * Mac address of the host. Mac address value MUST be protected by either single or dual quotes > ip4 : < * IP Address of the host > registration : < Registration URL to use for the host. If not set, default value will be applied > gateway : < Gateway to use for the host. If not set, default value will be applied > nameservers : < List of NS to use for the host. If not set, default value will be applied > Variables with * are mandatory, others are optional and might be skipped if not needed in your setup.","title":"Role Variables"},{"location":"roles/dhcp_configuration/#dependencies","text":"No dependency required for this role.","title":"Dependencies"},{"location":"roles/dhcp_configuration/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/dhcp_configuration/#generate-dhcpd-configuration-to-deploy-on-a-dhcp-server","text":"--- - name : Configure DHCP service on CloudVision hosts : dhcp_server gather_facts : false collection : - arista.cvp vars : ztp : default : registration : 'http://10.255.0.1/ztp/bootstrap' gateway : 10.255.0.3 nameservers : - '10.255.0.3' general : subnets : - network : 10.255.0.0 netmask : 255.255.255.0 gateway : 10.255.0.3 nameservers : - '10.255.0.3' start : 10.255.0.200 end : 10.255.0.250 lease_time : 300 clients : - name : DC1-SPINE1 mac : '0c:1d:c0:1d:62:01' ip4 : 10.255.0.11 - name : DC1-SPINE2 mac : '0c:1d:c0:1d:62:02' ip4 : 10.255.0.12 - name : DC1-LEAF1A mac : '0c:1d:c0:1d:62:11' ip4 : 10.255.0.13 tasks : - name : 'Execute DHCP configuration role' import_role : name : arista.cvp.dhcp_configuration vars : mode : offline output_dirs : '{{inventory}}'","title":"Generate DHCPD configuration to deploy on a DHCP server"},{"location":"roles/dhcp_configuration/#configure-server-to-act-as-ztp-server","text":"Below is a basic playbook running arista.cvp.dhcp_configuration role --- - name : Configure DHCP service on CloudVision hosts : dhcp_server gather_facts : true collection : - arista.cvp vars : ztp : default : registration : 'http://10.255.0.1/ztp/bootstrap' gateway : 10.255.0.3 nameservers : - '10.255.0.3' general : subnets : - network : 10.255.0.0 netmask : 255.255.255.0 gateway : 10.255.0.3 nameservers : - '10.255.0.3' start : 10.255.0.200 end : 10.255.0.250 lease_time : 300 clients : - name : DC1-SPINE1 mac : '0c:1d:c0:1d:62:01' ip4 : 10.255.0.11 - name : DC1-SPINE2 mac : '0c:1d:c0:1d:62:02' ip4 : 10.255.0.12 - name : DC1-LEAF1A mac : '0c:1d:c0:1d:62:11' ip4 : 10.255.0.13 tasks : - name : 'Execute DHCP configuration role' import_role : name : arista.cvp.dhcp_configuration Inventory is configured like below: --- all : children : CVP : hosts : dhcp_server : ansible_host : 1.1.1.1 ansible_user : user1 ansible_password : password1 ansible_become_password : password1 ansible_python_interpreter : $(which python3) If you are not using root user, configure ansible_become_password since role always use become: true . SSH connection is managed by paramiko .","title":"Configure Server to act as ZTP server"},{"location":"roles/dhcp_configuration/#license","text":"Project is published under Apache 2.0 License","title":"License"}]}